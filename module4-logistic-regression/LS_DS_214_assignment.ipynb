{"cells":[{"cell_type":"markdown","metadata":{},"source":["Lambda School Data Science\n","\n","*Unit 2, Sprint 1, Module 4*\n","\n","---"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"7IXUfiQ2UKj6"},"source":["# Logistic Regression\n","\n","\n","## Assignment ðŸŒ¯\n","\n","You'll use a [**dataset of 400+ burrito reviews**](https://srcole.github.io/100burritos/). How accurately can you predict whether a burrito is rated 'Great'?\n","\n","> We have developed a 10-dimensional system for rating the burritos in San Diego. ... Generate models for what makes a burrito great and investigate correlations in its dimensions.\n","\n","- [ ] Do train/validate/test split. Train on reviews from 2016 & earlier. Validate on 2017. Test on 2018 & later.\n","- [ ] Begin with baselines for classification.\n","- [ ] Use scikit-learn for logistic regression.\n","- [ ] Get your model's validation accuracy. (Multiple times if you try multiple iterations.)\n","- [ ] Get your model's test accuracy. (One time, at the end.)\n","- [ ] Commit your notebook to your fork of the GitHub repo.\n","\n","\n","## Stretch Goals\n","\n","- [ ] Add your own stretch goal(s) !\n","- [ ] Make exploratory visualizations.\n","- [ ] Do one-hot encoding.\n","- [ ] Do [feature scaling](https://scikit-learn.org/stable/modules/preprocessing.html).\n","- [ ] Get and plot your coefficients.\n","- [ ] Try [scikit-learn pipelines](https://scikit-learn.org/stable/modules/compose.html)."]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["%%capture\n","import sys\n","\n","# If you're on Colab:\n","if 'google.colab' or 'jupyter_client' in sys.modules:\n","    DATA_PATH = 'https://raw.githubusercontent.com/LambdaSchool/DS-Unit-2-Linear-Models/master/data/'\n","    !pip install category_encoders==2.*\n","\n","# If you're working locally:\n","else:\n","    DATA_PATH = '../data/'"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["# Load data downloaded from https://srcole.github.io/100burritos/\n","import pandas as pd\n","df = pd.read_csv(DATA_PATH+'burritos/burritos.csv')"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["pd.set_option('display.max_rows', 500)\n","pd.set_option('display.max_columns', 500)\n","pd.set_option('display.width', 1000)"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["# Derive binary classification target:\n","# We define a 'Great' burrito as having an\n","# overall rating of 4 or higher, on a 5 point scale.\n","# Drop unrated burritos.\n","df = df.dropna(subset=['overall'])\n","df['Great'] = df['overall'] >= 4"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["# Clean/combine the Burrito categories\n","df['Burrito'] = df['Burrito'].str.lower()\n","\n","california = df['Burrito'].str.contains('california')\n","asada = df['Burrito'].str.contains('asada')\n","surf = df['Burrito'].str.contains('surf')\n","carnitas = df['Burrito'].str.contains('carnitas')\n","\n","df.loc[california, 'Burrito'] = 'California'\n","df.loc[asada, 'Burrito'] = 'Asada'\n","df.loc[surf, 'Burrito'] = 'Surf & Turf'\n","df.loc[carnitas, 'Burrito'] = 'Carnitas'\n","df.loc[~california & ~asada & ~surf & ~carnitas, 'Burrito'] = 'Other'"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["# Drop some high cardinality categoricals\n","df = df.drop(columns=['Notes', 'Location', 'Reviewer', 'Address', 'URL', 'Neighborhood'])"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["# Drop some columns to prevent \"leakage\"\n","# I don't understand this line\n","df = df.drop(columns=['Rec', 'overall'])"]},{"cell_type":"markdown","metadata":{},"source":["### 1. Do train/validate/test split. Train on reviews from 2016 & earlier. Validate on 2017. Test on 2018 & later."]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"before count           421\nunique          169\ntop       8/30/2016\nfreq             29\nName: Date, dtype: object\nafter count                     421\nunique                    169\ntop       2016-08-30 00:00:00\nfreq                       29\nfirst     2011-05-16 00:00:00\nlast      2026-04-25 00:00:00\nName: Date, dtype: object\n"}],"source":["# change date string to datetime object\n","print('before', df['Date'].describe())\n","df['Date'] = pd.to_datetime(df['Date'], infer_datetime_format=True)\n","print('after', df['Date'].describe())"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"data":{"text/plain":"0   2016-01-18\n1   2016-01-24\n2   2016-01-24\n3   2016-01-24\n4   2016-01-27\nName: Date, dtype: datetime64[ns]"},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["df['Date'].head()"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"train 0   2016-01-18\n1   2016-01-24\n2   2016-01-24\n3   2016-01-24\n4   2016-01-27\nName: Date, dtype: datetime64[ns]\nvalidation 301   2017-01-04\n302   2017-01-04\n303   2017-01-07\n304   2017-01-07\n305   2017-01-10\nName: Date, dtype: datetime64[ns]\ntest 77    2026-04-25\n386   2018-01-02\n387   2018-01-09\n388   2018-01-12\n389   2018-01-12\nName: Date, dtype: datetime64[ns]\n"}],"source":["# split into train and validation set\n","# 1. train\n","cond_train = df['Date'] < '2017-01-01'\n","cond_valid = (df['Date'] > '2016-12-31') & (df['Date'] < '2018-01-01')\n","cond_test = (df['Date'] > '2017-12-31')\n","train = df[cond_train]\n","valid = df[cond_valid]\n","test = df[cond_test]\n","\n","print('train', train['Date'].head())\n","print('validation', valid['Date'].head())\n","print('test', test['Date'].head())"]},{"cell_type":"markdown","metadata":{},"source":["### 2. Begin with baselines for classification."]},{"cell_type":"code","execution_count":32,"metadata":{},"outputs":[{"data":{"text/plain":"False    0.590604\nTrue     0.409396\nName: Great, dtype: float64"},"execution_count":32,"metadata":{},"output_type":"execute_result"}],"source":["# determine majority rate whther great or not\n","target = 'Great'\n","y_train = train[target]\n","y_train.value_counts(normalize=True)"]},{"cell_type":"code","execution_count":41,"metadata":{},"outputs":[],"source":["# what if we guessed the majority rate for every predicition?\n","majority_rate = y_train.mode()[0]\n","y_pred_train = [majority_rate]*len(y_train)"]},{"cell_type":"code","execution_count":42,"metadata":{},"outputs":[{"data":{"text/plain":"0.5906040268456376"},"execution_count":42,"metadata":{},"output_type":"execute_result"}],"source":["# use a classification metric: accuracy\n","from sklearn.metrics import accuracy_score\n","accuracy_score(y_train, y_pred_train)"]},{"cell_type":"code","execution_count":43,"metadata":{},"outputs":[{"data":{"text/plain":"0.5529411764705883"},"execution_count":43,"metadata":{},"output_type":"execute_result"}],"source":["y_valid = valid[target]\n","y_pred = [majority_rate]*len(y_valid)\n","accuracy_score(y_valid, y_pred)"]},{"cell_type":"markdown","metadata":{},"source":["# Q: I don't understand what the baseline means\n","is it something like the wors prediction or accuracy that I start with? \n","\n","and we start beating it up with fancy skills?"]},{"cell_type":"markdown","metadata":{},"source":["### 3. Linear (Logistic) Regression\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":44,"metadata":{},"outputs":[{"data":{"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Yelp</th>\n      <th>Google</th>\n      <th>Cost</th>\n      <th>Hunger</th>\n      <th>Mass (g)</th>\n      <th>Density (g/mL)</th>\n      <th>Length</th>\n      <th>Circum</th>\n      <th>Volume</th>\n      <th>Tortilla</th>\n      <th>Temp</th>\n      <th>Meat</th>\n      <th>Fillings</th>\n      <th>Meat:filling</th>\n      <th>Uniformity</th>\n      <th>Salsa</th>\n      <th>Synergy</th>\n      <th>Wrap</th>\n      <th>Queso</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>71.000000</td>\n      <td>71.000000</td>\n      <td>292.000000</td>\n      <td>297.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>175.000000</td>\n      <td>174.000000</td>\n      <td>174.000000</td>\n      <td>298.000000</td>\n      <td>283.000000</td>\n      <td>288.000000</td>\n      <td>297.000000</td>\n      <td>292.000000</td>\n      <td>296.000000</td>\n      <td>278.000000</td>\n      <td>296.000000</td>\n      <td>296.000000</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>3.897183</td>\n      <td>4.142254</td>\n      <td>6.896781</td>\n      <td>3.445286</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>19.829886</td>\n      <td>22.042241</td>\n      <td>0.770920</td>\n      <td>3.472315</td>\n      <td>3.706360</td>\n      <td>3.551215</td>\n      <td>3.519024</td>\n      <td>3.528870</td>\n      <td>3.395946</td>\n      <td>3.324640</td>\n      <td>3.540203</td>\n      <td>3.955068</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>0.478680</td>\n      <td>0.371738</td>\n      <td>1.211412</td>\n      <td>0.852150</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2.081275</td>\n      <td>1.685043</td>\n      <td>0.137833</td>\n      <td>0.797606</td>\n      <td>0.991897</td>\n      <td>0.869483</td>\n      <td>0.850348</td>\n      <td>1.040457</td>\n      <td>1.089044</td>\n      <td>0.971226</td>\n      <td>0.922426</td>\n      <td>1.167341</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>2.500000</td>\n      <td>2.900000</td>\n      <td>2.990000</td>\n      <td>0.500000</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>15.000000</td>\n      <td>17.000000</td>\n      <td>0.400000</td>\n      <td>1.400000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.500000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>3.500000</td>\n      <td>4.000000</td>\n      <td>6.250000</td>\n      <td>3.000000</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>18.500000</td>\n      <td>21.000000</td>\n      <td>0.662500</td>\n      <td>3.000000</td>\n      <td>3.000000</td>\n      <td>3.000000</td>\n      <td>3.000000</td>\n      <td>3.000000</td>\n      <td>2.500000</td>\n      <td>2.500000</td>\n      <td>3.000000</td>\n      <td>3.500000</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>4.000000</td>\n      <td>4.200000</td>\n      <td>6.850000</td>\n      <td>3.500000</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>19.500000</td>\n      <td>22.000000</td>\n      <td>0.750000</td>\n      <td>3.500000</td>\n      <td>4.000000</td>\n      <td>3.500000</td>\n      <td>3.500000</td>\n      <td>4.000000</td>\n      <td>3.500000</td>\n      <td>3.500000</td>\n      <td>3.750000</td>\n      <td>4.000000</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>4.000000</td>\n      <td>4.400000</td>\n      <td>7.500000</td>\n      <td>4.000000</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>21.000000</td>\n      <td>23.000000</td>\n      <td>0.870000</td>\n      <td>4.000000</td>\n      <td>4.500000</td>\n      <td>4.000000</td>\n      <td>4.000000</td>\n      <td>4.000000</td>\n      <td>4.000000</td>\n      <td>4.000000</td>\n      <td>4.000000</td>\n      <td>5.000000</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>4.500000</td>\n      <td>4.900000</td>\n      <td>11.950000</td>\n      <td>5.000000</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>26.000000</td>\n      <td>27.000000</td>\n      <td>1.240000</td>\n      <td>5.000000</td>\n      <td>5.000000</td>\n      <td>5.000000</td>\n      <td>5.000000</td>\n      <td>5.000000</td>\n      <td>5.000000</td>\n      <td>5.000000</td>\n      <td>5.000000</td>\n      <td>5.000000</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>","text/plain":"            Yelp     Google        Cost      Hunger  Mass (g)  Density (g/mL)      Length      Circum      Volume    Tortilla        Temp        Meat    Fillings  Meat:filling  Uniformity       Salsa     Synergy        Wrap  Queso\ncount  71.000000  71.000000  292.000000  297.000000       0.0             0.0  175.000000  174.000000  174.000000  298.000000  283.000000  288.000000  297.000000    292.000000  296.000000  278.000000  296.000000  296.000000    0.0\nmean    3.897183   4.142254    6.896781    3.445286       NaN             NaN   19.829886   22.042241    0.770920    3.472315    3.706360    3.551215    3.519024      3.528870    3.395946    3.324640    3.540203    3.955068    NaN\nstd     0.478680   0.371738    1.211412    0.852150       NaN             NaN    2.081275    1.685043    0.137833    0.797606    0.991897    0.869483    0.850348      1.040457    1.089044    0.971226    0.922426    1.167341    NaN\nmin     2.500000   2.900000    2.990000    0.500000       NaN             NaN   15.000000   17.000000    0.400000    1.400000    1.000000    1.000000    1.000000      0.500000    1.000000    0.000000    1.000000    0.000000    NaN\n25%     3.500000   4.000000    6.250000    3.000000       NaN             NaN   18.500000   21.000000    0.662500    3.000000    3.000000    3.000000    3.000000      3.000000    2.500000    2.500000    3.000000    3.500000    NaN\n50%     4.000000   4.200000    6.850000    3.500000       NaN             NaN   19.500000   22.000000    0.750000    3.500000    4.000000    3.500000    3.500000      4.000000    3.500000    3.500000    3.750000    4.000000    NaN\n75%     4.000000   4.400000    7.500000    4.000000       NaN             NaN   21.000000   23.000000    0.870000    4.000000    4.500000    4.000000    4.000000      4.000000    4.000000    4.000000    4.000000    5.000000    NaN\nmax     4.500000   4.900000   11.950000    5.000000       NaN             NaN   26.000000   27.000000    1.240000    5.000000    5.000000    5.000000    5.000000      5.000000    5.000000    5.000000    5.000000    5.000000    NaN"},"execution_count":44,"metadata":{},"output_type":"execute_result"}],"source":["train.describe()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# 1. Import estimator class\n","from sklearn.linear_model import LinearRegression\n","\n","# 2. Instantiate this class\n","linear_reg = LinearRegression()\n","\n","# 3. Arrange X feature matrices (already did y target vectors)\n","features = []"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.5-final"}},"nbformat":4,"nbformat_minor":1}