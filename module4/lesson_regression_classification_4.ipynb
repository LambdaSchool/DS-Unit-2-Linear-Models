{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eSA4DnL3itZG"
   },
   "source": [
    "Lambda School Data Science, Unit 2: Predictive Modeling\n",
    "\n",
    "# Regression & Classification, Module 4\n",
    "\n",
    "\n",
    "#### Objectives\n",
    "- begin with baselines for **classification**\n",
    "- use classification metric: **accuracy**\n",
    "- do **train/validate/test** split\n",
    "- use scikit-learn for **logistic regression**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mBrdnrAiSlAh"
   },
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eYW8zY1Zn_h-"
   },
   "source": [
    "#### Get started on Kaggle\n",
    "1. [Sign up for a Kaggle account](https://www.kaggle.com/), if you don’t already have one. \n",
    "2. Go to our Kaggle InClass competition website. You will be given the URL in Slack.\n",
    "3. Go to the Rules page. Accept the rules of the competition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vuoAHQDuSp1A"
   },
   "source": [
    "#### If you're using [Anaconda](https://www.anaconda.com/distribution/) locally\n",
    "\n",
    "Install required Python packages, if you haven't already:\n",
    "\n",
    "- [category_encoders](http://contrib.scikit-learn.org/categorical-encoding/), version >= 2.0\n",
    "- [pandas-profiling](https://github.com/pandas-profiling/pandas-profiling), version >= 2.0\n",
    "\n",
    "```\n",
    "conda install -c conda-forge category_encoders plotly\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wqHsEt-vTArf"
   },
   "outputs": [],
   "source": [
    "# If you're in Colab...\n",
    "import os, sys\n",
    "in_colab = 'google.colab' in sys.modules\n",
    "\n",
    "if in_colab:\n",
    "    # Install required python packages:\n",
    "    # category_encoders, version >= 2.0\n",
    "    # pandas-profiling, version >= 2.0\n",
    "    # plotly, version >= 4.0\n",
    "    !pip install --upgrade category_encoders pandas-profiling plotly\n",
    "    \n",
    "    # Pull files from Github repo\n",
    "    os.chdir('/content')\n",
    "    !git init .\n",
    "    !git remote add origin https://github.com/LambdaSchool/DS-Unit-2-Regression-Classification.git\n",
    "    !git pull origin master\n",
    "    \n",
    "    # Change into directory for module\n",
    "    os.chdir('module4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CDs_QjVJpaSP"
   },
   "outputs": [],
   "source": [
    "# Ignore this Numpy warning when using Plotly Express:\n",
    "# FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore', category=FutureWarning, module='numpy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ph7_ka3DrjzA"
   },
   "source": [
    "## Read data\n",
    "\n",
    "The files are in the GitHub repository, in the `data/tanzania` folder:\n",
    "\n",
    " - `train_features.csv` : the training set features\n",
    " - `train_labels.csv` : the training set labels\n",
    " - `test_features.csv` : the test set features\n",
    " - `sample_submission.csv` :  a sample submission file in the correct format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3_1DyzxZje5X"
   },
   "source": [
    "####  Alternative option to get data & make submissions: Kaggle API\n",
    "\n",
    "1. Go to our Kaggle InClass competition webpage. Accept the rules of the competiton.\n",
    "\n",
    "2. [Follow these instructions](https://github.com/Kaggle/kaggle-api#api-credentials) to create a Kaggle “API Token” and download your `kaggle.json` file.\n",
    "\n",
    "3. Put `kaggle.json` in the correct location.\n",
    "\n",
    "  - If you're using Anaconda, put the file in the directory specified in the [instructions](https://github.com/Kaggle/kaggle-api#api-credentials).\n",
    "\n",
    "  - If you're using Google Colab, upload the file to your Google Drive, and run this cell:\n",
    "\n",
    "  ```\n",
    "  from google.colab import drive\n",
    "  drive.mount('/content/drive')\n",
    "  %env KAGGLE_CONFIG_DIR=/content/drive/My Drive/\n",
    "  ```\n",
    "\n",
    "4. [Install the Kaggle API package](https://github.com/Kaggle/kaggle-api#installation).\n",
    "\n",
    "5. [Use Kaggle API to download competition files](https://github.com/Kaggle/kaggle-api#download-competition-files).\n",
    "\n",
    "6. [Use Kaggle API to submit to competition](https://github.com/Kaggle/kaggle-api#submit-to-a-competition)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nhiIa4x_pEPD"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_features = pd.read_csv('../data/tanzania/train_features.csv')\n",
    "train_labels = pd.read_csv('../data/tanzania/train_labels.csv')\n",
    "test_features = pd.read_csv('../data/tanzania/test_features.csv')\n",
    "sample_submission = pd.read_csv('../data/tanzania/sample_submission.csv')\n",
    "\n",
    "assert train_features.shape == (59400, 40)\n",
    "assert train_labels.shape == (59400, 2)\n",
    "assert test_features.shape == (14358, 40)\n",
    "assert sample_submission.shape == (14358, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_jUjpFduLlVt"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ScT5oOhCraOO"
   },
   "source": [
    "### Features\n",
    "\n",
    "Your goal is to predict the operating condition of a waterpoint for each record in the dataset. You are provided the following set of information about the waterpoints:\n",
    "\n",
    "- `amount_tsh` : Total static head (amount water available to waterpoint)\n",
    "- `date_recorded` : The date the row was entered\n",
    "- `funder` : Who funded the well\n",
    "- `gps_height` : Altitude of the well\n",
    "- `installer` : Organization that installed the well\n",
    "- `longitude` : GPS coordinate\n",
    "- `latitude` : GPS coordinate\n",
    "- `wpt_name` : Name of the waterpoint if there is one\n",
    "- `num_private` :  \n",
    "- `basin` : Geographic water basin\n",
    "- `subvillage` : Geographic location\n",
    "- `region` : Geographic location\n",
    "- `region_code` : Geographic location (coded)\n",
    "- `district_code` : Geographic location (coded)\n",
    "- `lga` : Geographic location\n",
    "- `ward` : Geographic location\n",
    "- `population` : Population around the well\n",
    "- `public_meeting` : True/False\n",
    "- `recorded_by` : Group entering this row of data\n",
    "- `scheme_management` : Who operates the waterpoint\n",
    "- `scheme_name` : Who operates the waterpoint\n",
    "- `permit` : If the waterpoint is permitted\n",
    "- `construction_year` : Year the waterpoint was constructed\n",
    "- `extraction_type` : The kind of extraction the waterpoint uses\n",
    "- `extraction_type_group` : The kind of extraction the waterpoint uses\n",
    "- `extraction_type_class` : The kind of extraction the waterpoint uses\n",
    "- `management` : How the waterpoint is managed\n",
    "- `management_group` : How the waterpoint is managed\n",
    "- `payment` : What the water costs\n",
    "- `payment_type` : What the water costs\n",
    "- `water_quality` : The quality of the water\n",
    "- `quality_group` : The quality of the water\n",
    "- `quantity` : The quantity of water\n",
    "- `quantity_group` : The quantity of water\n",
    "- `source` : The source of the water\n",
    "- `source_type` : The source of the water\n",
    "- `source_class` : The source of the water\n",
    "- `waterpoint_type` : The kind of waterpoint\n",
    "- `waterpoint_type_group` : The kind of waterpoint\n",
    "\n",
    "### Labels\n",
    "\n",
    "There are three possible values:\n",
    "\n",
    "- `functional` : the waterpoint is operational and there are no repairs needed\n",
    "- `functional needs repair` : the waterpoint is operational, but needs repairs\n",
    "- `non functional` : the waterpoint is not operational"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LIWeFmyWswtB"
   },
   "source": [
    "## Why doesn't Kaggle give you labels for the test set?\n",
    "\n",
    "#### Rachel Thomas, [How (and why) to create a good validation set](https://www.fast.ai/2017/11/13/validation-sets/)\n",
    "\n",
    "> One great thing about Kaggle competitions is that they force you to think about validation sets more rigorously (in order to do well). For those who are new to Kaggle, it is a platform that hosts machine learning competitions. Kaggle typically breaks the data into two sets you can download:\n",
    "\n",
    "> 1. a **training set**, which includes the _independent variables_, as well as the _dependent variable_ (what you are trying to predict).\n",
    "\n",
    "> 2. a **test set**, which just has the _independent variables_. You will make predictions for the test set, which you can submit to Kaggle and get back a score of how well you did.\n",
    "\n",
    "> This is the basic idea needed to get started with machine learning, but to do well, there is a bit more complexity to understand. You will want to create your own training and validation sets (by splitting the Kaggle “training” data). You will just use your smaller training set (a subset of Kaggle’s training data) for building your model, and you can evaluate it on your validation set (also a subset of Kaggle’s training data) before you submit to Kaggle.\n",
    "\n",
    "> The most important reason for this is that Kaggle has split the test data into two sets: for the public and private leaderboards. The score you see on the public leaderboard is just for a subset of your predictions (and you don’t know which subset!). How your predictions fare on the private leaderboard won’t be revealed until the end of the competition. The reason this is important is that you could end up overfitting to the public leaderboard and you wouldn’t realize it until the very end when you did poorly on the private leaderboard. Using a good validation set can prevent this. You can check if your validation set is any good by seeing if your model has similar scores on it to compared with on the Kaggle test set. ...\n",
    "\n",
    "> Understanding these distinctions is not just useful for Kaggle. In any predictive machine learning project, you want your model to be able to perform well on new data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "simUkHjSs2t_"
   },
   "source": [
    "## Why care about model validation?\n",
    "\n",
    "#### Rachel Thomas, [How (and why) to create a good validation set](https://www.fast.ai/2017/11/13/validation-sets/)\n",
    "\n",
    "> An all-too-common scenario: a seemingly impressive machine learning model is a complete failure when implemented in production. The fallout includes leaders who are now skeptical of machine learning and reluctant to try it again. How can this happen?\n",
    "\n",
    "> One of the most likely culprits for this disconnect between results in development vs results in production is a poorly chosen validation set (or even worse, no validation set at all). \n",
    "\n",
    "#### Owen Zhang, [Winning Data Science Competitions](https://www.slideshare.net/OwenZhang2/tips-for-data-science-competitions/8)\n",
    "\n",
    "> Good validation is _more important_ than good models. \n",
    "\n",
    "#### James, Witten, Hastie, Tibshirani, [An Introduction to Statistical Learning](http://www-bcf.usc.edu/~gareth/ISL/), Chapter 2.2, Assessing Model Accuracy\n",
    "\n",
    "> In general, we do not really care how well the method works training on the training data. Rather, _we are interested in the accuracy of the predictions that we obtain when we apply our method to previously unseen test data._ Why is this what we care about? \n",
    "\n",
    "> Suppose that we are interested test data in developing an algorithm to predict a stock’s price based on previous stock returns. We can train the method using stock returns from the past 6 months. But we don’t really care how well our method predicts last week’s stock price. We instead care about how well it will predict tomorrow’s price or next month’s price. \n",
    "\n",
    "> On a similar note, suppose that we have clinical measurements (e.g. weight, blood pressure, height, age, family history of disease) for a number of patients, as well as information about whether each patient has diabetes. We can use these patients to train a statistical learning method to predict risk of diabetes based on clinical measurements. In practice, we want this method to accurately predict diabetes risk for _future patients_ based on their clinical measurements. We are not very interested in whether or not the method accurately predicts diabetes risk for patients used to train the model, since we already know which of those patients have diabetes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LVZMzBqwvTdD"
   },
   "source": [
    "## Why hold out an independent test set?\n",
    "\n",
    "#### Owen Zhang, [Winning Data Science Competitions](https://www.slideshare.net/OwenZhang2/tips-for-data-science-competitions)\n",
    "\n",
    "> There are many ways to overfit. Beware of \"multiple comparison fallacy.\" There is a cost in \"peeking at the answer.\"\n",
    "\n",
    "> Good validation is _more important_ than good models. Simple training/validation split is _not_ enough. When you looked at your validation result for the Nth time, you are training models on it.\n",
    "\n",
    "> If possible, have \"holdout\" dataset that you do not touch at all during model build process. This includes feature extraction, etc.\n",
    "\n",
    "> What if holdout result is bad? Be brave and scrap the project.\n",
    "\n",
    "#### Hastie, Tibshirani, and Friedman, [The Elements of Statistical Learning](http://statweb.stanford.edu/~tibs/ElemStatLearn/), Chapter 7: Model Assessment and Selection\n",
    "\n",
    "> If we are in a data-rich situation, the best approach is to randomly divide the dataset into three parts: a training set, a validation set, and a test set. The training set is used to fit the models; the validation set is used to estimate prediction error for model selection; the test set is used for assessment of the generalization error of the final chosen model. Ideally, the test set should be kept in a \"vault,\" and be brought out only at the end of the data analysis. Suppose instead that we use the test-set repeatedly, choosing the model with the smallest test-set error. Then the test set error of the final chosen model will underestimate the true test error, sometimes substantially.\n",
    "\n",
    "#### Andreas Mueller and Sarah Guido, [Introduction to Machine Learning with Python](https://books.google.com/books?id=1-4lDQAAQBAJ&pg=PA270)\n",
    "\n",
    "> The distinction between the training set, validation set, and test set is fundamentally important to applying machine learning methods in practice. Any choices made based on the test set accuracy \"leak\" information from the test set into the model. Therefore, it is important to keep a separate test set, which is only used for the final evaluation. It is good practice to do all exploratory analysis and model selection using the combination of a training and a validation set, and reserve the test set for a final evaluation - this is even true for exploratory visualization. Strictly speaking, evaluating more than one model on the test set and choosing the better of the two will result in an overly optimistic estimate of how accurate the model is.\n",
    "\n",
    "#### Hadley Wickham, [R for Data Science](https://r4ds.had.co.nz/model-intro.html#hypothesis-generation-vs.hypothesis-confirmation)\n",
    "\n",
    "> There is a pair of ideas that you must understand in order to do inference correctly:\n",
    "\n",
    "> 1. Each observation can either be used for exploration or confirmation, not both.\n",
    "\n",
    "> 2. You can use an observation as many times as you like for exploration, but you can only use it once for confirmation. As soon as you use an observation twice, you’ve switched from confirmation to exploration.\n",
    "\n",
    "> This is necessary because to confirm a hypothesis you must use data independent of the data that you used to generate the hypothesis. Otherwise you will be over optimistic. There is absolutely nothing wrong with exploration, but you should never sell an exploratory analysis as a confirmatory analysis because it is fundamentally misleading.\n",
    "\n",
    "> If you are serious about doing an confirmatory analysis, one approach is to split your data into three pieces before you begin the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n-nP5AUBtAsk"
   },
   "source": [
    "### Why begin with baselines?\n",
    "\n",
    "[My mentor](https://www.linkedin.com/in/jason-sanchez-62093847/) [taught me](https://youtu.be/0GrciaGYzV0?t=40s):\n",
    "\n",
    ">***Your first goal should always, always, always be getting a generalized prediction as fast as possible.*** You shouldn't spend a lot of time trying to tune your model, trying to add features, trying to engineer features, until you've actually gotten one prediction, at least. \n",
    "\n",
    "> The reason why that's a really good thing is because then ***you'll set a benchmark*** for yourself, and you'll be able to directly see how much effort you put in translates to a better prediction. \n",
    "\n",
    "> What you'll find by working on many models: some effort you put in, actually has very little effect on how well your final model does at predicting new observations. Whereas some very easy changes actually have a lot of effect. And so you get better at allocating your time more effectively.\n",
    "\n",
    "My mentor's advice is echoed and elaborated in several sources:\n",
    "\n",
    "[Always start with a stupid model, no exceptions](https://blog.insightdatascience.com/always-start-with-a-stupid-model-no-exceptions-3a22314b9aaa)\n",
    "\n",
    "> Why start with a baseline? A baseline will take you less than 1/10th of the time, and could provide up to 90% of the results. A baseline puts a more complex model into context. Baselines are easy to deploy.\n",
    "\n",
    "[Measure Once, Cut Twice: Moving Towards Iteration in Data Science](https://blog.datarobot.com/measure-once-cut-twice-moving-towards-iteration-in-data-science)\n",
    "\n",
    "> The iterative approach in data science starts with emphasizing the importance of getting to a first model quickly, rather than starting with the variables and features. Once the first model is built, the work then steadily focuses on continual improvement.\n",
    "\n",
    "[*Data Science for Business*](https://books.google.com/books?id=4ZctAAAAQBAJ&pg=PT276), Chapter 7.3: Evaluation, Baseline Performance, and Implications for Investments in Data\n",
    "\n",
    "> *Consider carefully what would be a reasonable baseline against which to compare model performance.* This is important for the data science team in order to understand whether they indeed are improving performance, and is equally important for demonstrating to stakeholders that mining the data has added value.\n",
    "\n",
    "### What does baseline mean?\n",
    "\n",
    "Baseline is an overloaded term, as you can see in the links above. Baseline has multiple meanings:\n",
    "\n",
    "#### The score you'd get by guessing\n",
    "\n",
    "> A baseline for classification can be the most common class in the training dataset.\n",
    "\n",
    "> A baseline for regression can be the mean of the training labels. \n",
    "\n",
    "> A baseline for time-series regressions can be the value from the previous timestep. —[Will Koehrsen](https://twitter.com/koehrsen_will/status/1088863527778111488)\n",
    "\n",
    "#### Fast, first models that beat guessing\n",
    "\n",
    "What my mentor was talking about.\n",
    "\n",
    "#### Complete, tuned \"simpler\" model\n",
    "\n",
    "Can be simpler mathematically and computationally. For example, Logistic Regression versus Deep Learning.\n",
    "\n",
    "Or can be simpler for the data scientist, with less work. For example, a model with less feature engineering versus a model with more feature engineering.\n",
    "\n",
    "#### Minimum performance that \"matters\"\n",
    "\n",
    "To go to production and get business value.\n",
    "\n",
    "#### Human-level performance \n",
    "\n",
    "Your goal may to be match, or nearly match, human performance, but with better speed, cost, or consistency.\n",
    "\n",
    "Or your goal may to be exceed human performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tG74jmbKrsj-"
   },
   "source": [
    "## Begin with baselines for classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QKCDx07WxXZj"
   },
   "source": [
    "### Get majority class baseline\n",
    "\n",
    "[Will Koehrsen](https://twitter.com/koehrsen_will/status/1088863527778111488)\n",
    "\n",
    "> A baseline for classification can be the most common class in the training dataset.\n",
    "\n",
    "[*Data Science for Business*](https://books.google.com/books?id=4ZctAAAAQBAJ&pg=PT276), Chapter 7.3: Evaluation, Baseline Performance, and Implications for Investments in Data\n",
    "\n",
    "> For classification tasks, one good baseline is the _majority classifier_, a naive classifier that always chooses the majority class of the training dataset (see Note: Base rate in Holdout Data and Fitting Graphs). This may seem like advice so obvious it can be passed over quickly, but it is worth spending an extra moment here. There are many cases where smart, analytical people have been tripped up in skipping over this basic comparison. For example, an analyst may see a classification accuracy of 94% from her classifier and conclude that it is doing fairly well—when in fact only 6% of the instances are positive. So, the simple majority prediction classifier also would have an accuracy of 94%. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nRnL7Bw12YZo"
   },
   "source": [
    "#### Determine majority class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6D6UZ1XJxTpj"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "functional                 0.543081\n",
       "non functional             0.384242\n",
       "functional needs repair    0.072677\n",
       "Name: status_group, dtype: float64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = train_labels['status_group']\n",
    "y_train.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Hl8qcAgp2bKC"
   },
   "source": [
    "#### What if we guessed the majority class for every prediction?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sNhv3xPc2GHl"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59400"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "majority_class = y_train.mode()[0]\n",
    "y_pred = [majority_class] * len(y_train)\n",
    "len(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2WWkumm3rwdb"
   },
   "source": [
    "## Use classification metric: accuracy\n",
    "\n",
    "#### [_Classification metrics are different from regression metrics!_](https://scikit-learn.org/stable/modules/model_evaluation.html)\n",
    "- Don't use _regression_ metrics to evaluate _classification_ tasks.\n",
    "- Don't use _classification_ metrics to evaluate _regression_ tasks.\n",
    "\n",
    "[Accuracy](https://scikit-learn.org/stable/modules/model_evaluation.html#accuracy-score) is a common metric for classification. Accuracy is the [\"proportion of correct classifications\"](https://en.wikipedia.org/wiki/Confusion_matrix): the number of correct predictions divided by the total number of predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "p7TYYqJT28f1"
   },
   "source": [
    "#### What is the baseline accuracy if we guessed the majority class for every prediction?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IhhM1vAd2s0b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.543080808080808"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_train, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y2OLlsMar1c3"
   },
   "source": [
    "## Do train/validate/test split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Pq01q_kp3QKd"
   },
   "source": [
    "#### Rachel Thomas, [How (and why) to create a good validation set](https://www.fast.ai/2017/11/13/validation-sets/)\n",
    "\n",
    "> You will want to create your own training and validation sets (by splitting the Kaggle “training” data). You will just use your smaller training set (a subset of Kaggle’s training data) for building your model, and you can evaluate it on your validation set (also a subset of Kaggle’s training data) before you submit to Kaggle.\n",
    "\n",
    "#### Sebastian Raschka, [Model Evaluation](https://sebastianraschka.com/blog/2018/model-evaluation-selection-part4.html)\n",
    "\n",
    "> Since “a picture is worth a thousand words,” I want to conclude with a figure (shown below) that summarizes my personal recommendations ...\n",
    "\n",
    "<img src=\"https://sebastianraschka.com/images/blog/2018/model-evaluation-selection-part4/model-eval-conclusions.jpg\" width=\"600\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M1tGjw9_4u0r"
   },
   "source": [
    "\n",
    "Usually, we want to do **\"Model selection (hyperparameter optimization) _and_ performance estimation.\"**\n",
    "\n",
    "Therefore, we use **\"3-way holdout method (train/validation/test split)\"** or we use **\"cross-validation with independent test set.\"**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1JkSL6K14ry6"
   },
   "source": [
    "#### We have two options for where we choose to split:\n",
    "- Time\n",
    "- Random\n",
    "\n",
    "To split on time, we can use pandas.\n",
    "\n",
    "To split randomly, we can use the [**`sklearn.model_selection.train_test_split`**](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "86bG7yPe5aXI"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((59400, 40), (59400,))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train = train_features\n",
    "y_train = train_labels['status_group']\n",
    "\n",
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((47520, 40), (11880, 40), (47520,), (11880,))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train, y_train, train_size = 0.80, test_size = 0.20,\n",
    "    stratify = y_train, random_state = 42\n",
    ")\n",
    "\n",
    "X_train.shape, X_val.shape, y_train.shape, y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "functional                 0.543077\n",
       "non functional             0.384238\n",
       "functional needs repair    0.072685\n",
       "Name: status_group, dtype: float64"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kOdIbMMCr4Nc"
   },
   "source": [
    "## Use scikit-learn for logistic regression\n",
    "- [sklearn.linear_model.LogisticRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)\n",
    "- Wikipedia, [Logistic regression](https://en.wikipedia.org/wiki/Logistic_regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RIiTQPQ_8bDX"
   },
   "source": [
    "### Begin with baselines: fast, first models\n",
    "\n",
    "#### Drop non-numeric features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OEUujvzH7pBO"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>amount_tsh</th>\n",
       "      <th>gps_height</th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>num_private</th>\n",
       "      <th>region_code</th>\n",
       "      <th>district_code</th>\n",
       "      <th>population</th>\n",
       "      <th>construction_year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>43360</th>\n",
       "      <td>72938</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>33.542898</td>\n",
       "      <td>-9.174777</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7263</th>\n",
       "      <td>65358</td>\n",
       "      <td>500.0</td>\n",
       "      <td>2049</td>\n",
       "      <td>34.665760</td>\n",
       "      <td>-9.308548</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>175</td>\n",
       "      <td>2008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2486</th>\n",
       "      <td>469</td>\n",
       "      <td>25.0</td>\n",
       "      <td>290</td>\n",
       "      <td>38.238568</td>\n",
       "      <td>-6.179919</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>2300</td>\n",
       "      <td>2010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>313</th>\n",
       "      <td>1298</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>30.716727</td>\n",
       "      <td>-1.289055</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52726</th>\n",
       "      <td>27001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>35.389331</td>\n",
       "      <td>-6.399942</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8558</th>\n",
       "      <td>41546</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1295</td>\n",
       "      <td>31.214583</td>\n",
       "      <td>-8.431428</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>1986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2559</th>\n",
       "      <td>16230</td>\n",
       "      <td>20000.0</td>\n",
       "      <td>1515</td>\n",
       "      <td>36.696700</td>\n",
       "      <td>-3.337926</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>150</td>\n",
       "      <td>1995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54735</th>\n",
       "      <td>10307</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>36.292724</td>\n",
       "      <td>-5.177333</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25763</th>\n",
       "      <td>37145</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>32.877248</td>\n",
       "      <td>-8.925921</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44540</th>\n",
       "      <td>49234</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>33.014412</td>\n",
       "      <td>-3.115869</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28603</th>\n",
       "      <td>52130</td>\n",
       "      <td>0.0</td>\n",
       "      <td>286</td>\n",
       "      <td>38.950769</td>\n",
       "      <td>-10.889023</td>\n",
       "      <td>0</td>\n",
       "      <td>90</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "      <td>1985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4372</th>\n",
       "      <td>16650</td>\n",
       "      <td>20.0</td>\n",
       "      <td>17</td>\n",
       "      <td>39.020094</td>\n",
       "      <td>-5.346661</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>300</td>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30666</th>\n",
       "      <td>36612</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1404</td>\n",
       "      <td>36.694357</td>\n",
       "      <td>-3.035552</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>200</td>\n",
       "      <td>2001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6431</th>\n",
       "      <td>19057</td>\n",
       "      <td>50.0</td>\n",
       "      <td>-12</td>\n",
       "      <td>38.890990</td>\n",
       "      <td>-6.444754</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>2008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57420</th>\n",
       "      <td>4085</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>31.746292</td>\n",
       "      <td>-1.253936</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1373</th>\n",
       "      <td>12040</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1466</td>\n",
       "      <td>37.501573</td>\n",
       "      <td>-3.277989</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>15</td>\n",
       "      <td>1972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2026</th>\n",
       "      <td>21481</td>\n",
       "      <td>6500.0</td>\n",
       "      <td>1969</td>\n",
       "      <td>35.371400</td>\n",
       "      <td>-4.470802</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>2</td>\n",
       "      <td>162</td>\n",
       "      <td>2003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58977</th>\n",
       "      <td>69234</td>\n",
       "      <td>0.0</td>\n",
       "      <td>319</td>\n",
       "      <td>38.018784</td>\n",
       "      <td>-9.568769</td>\n",
       "      <td>0</td>\n",
       "      <td>80</td>\n",
       "      <td>43</td>\n",
       "      <td>250</td>\n",
       "      <td>2006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41101</th>\n",
       "      <td>48982</td>\n",
       "      <td>500.0</td>\n",
       "      <td>884</td>\n",
       "      <td>29.660968</td>\n",
       "      <td>-4.818648</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>1500</td>\n",
       "      <td>1985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10019</th>\n",
       "      <td>53992</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1472</td>\n",
       "      <td>35.714295</td>\n",
       "      <td>-3.314773</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>30</td>\n",
       "      <td>150</td>\n",
       "      <td>1994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5103</th>\n",
       "      <td>3658</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1274</td>\n",
       "      <td>30.241727</td>\n",
       "      <td>-4.182626</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>4210</td>\n",
       "      <td>1996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36712</th>\n",
       "      <td>45331</td>\n",
       "      <td>25.0</td>\n",
       "      <td>1551</td>\n",
       "      <td>31.023716</td>\n",
       "      <td>-7.490116</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29670</th>\n",
       "      <td>72431</td>\n",
       "      <td>0.0</td>\n",
       "      <td>292</td>\n",
       "      <td>35.944307</td>\n",
       "      <td>-8.525588</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>500</td>\n",
       "      <td>1980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54588</th>\n",
       "      <td>64881</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>32.580108</td>\n",
       "      <td>-3.549380</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31867</th>\n",
       "      <td>10831</td>\n",
       "      <td>200.0</td>\n",
       "      <td>217</td>\n",
       "      <td>36.679869</td>\n",
       "      <td>-8.157754</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>280</td>\n",
       "      <td>1979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12090</th>\n",
       "      <td>25701</td>\n",
       "      <td>0.0</td>\n",
       "      <td>312</td>\n",
       "      <td>38.975985</td>\n",
       "      <td>-10.528595</td>\n",
       "      <td>0</td>\n",
       "      <td>90</td>\n",
       "      <td>33</td>\n",
       "      <td>30</td>\n",
       "      <td>2010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48300</th>\n",
       "      <td>13557</td>\n",
       "      <td>50.0</td>\n",
       "      <td>983</td>\n",
       "      <td>35.451872</td>\n",
       "      <td>-10.640641</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16754</th>\n",
       "      <td>475</td>\n",
       "      <td>2200.0</td>\n",
       "      <td>1795</td>\n",
       "      <td>34.815906</td>\n",
       "      <td>-9.656551</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>40</td>\n",
       "      <td>2005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50582</th>\n",
       "      <td>44263</td>\n",
       "      <td>20.0</td>\n",
       "      <td>716</td>\n",
       "      <td>35.889997</td>\n",
       "      <td>-7.148531</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9136</th>\n",
       "      <td>50490</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1276</td>\n",
       "      <td>30.555693</td>\n",
       "      <td>-3.626554</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>2005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26450</th>\n",
       "      <td>25540</td>\n",
       "      <td>5.0</td>\n",
       "      <td>976</td>\n",
       "      <td>37.245432</td>\n",
       "      <td>-3.340166</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>50</td>\n",
       "      <td>1999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56373</th>\n",
       "      <td>56963</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>33.263250</td>\n",
       "      <td>-2.561022</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4595</th>\n",
       "      <td>74226</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1597</td>\n",
       "      <td>34.671798</td>\n",
       "      <td>-1.861973</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "      <td>400</td>\n",
       "      <td>1994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35771</th>\n",
       "      <td>18487</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>33.813449</td>\n",
       "      <td>-9.120584</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41564</th>\n",
       "      <td>22447</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1241</td>\n",
       "      <td>30.030766</td>\n",
       "      <td>-4.831457</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>750</td>\n",
       "      <td>1984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30571</th>\n",
       "      <td>20930</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>31.678456</td>\n",
       "      <td>-2.406259</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25961</th>\n",
       "      <td>57869</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1325</td>\n",
       "      <td>34.543563</td>\n",
       "      <td>-5.059194</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21280</th>\n",
       "      <td>28898</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1251</td>\n",
       "      <td>35.014961</td>\n",
       "      <td>-5.017372</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>230</td>\n",
       "      <td>2000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40122</th>\n",
       "      <td>38200</td>\n",
       "      <td>3000.0</td>\n",
       "      <td>979</td>\n",
       "      <td>31.074871</td>\n",
       "      <td>-7.094016</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>500</td>\n",
       "      <td>2006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42270</th>\n",
       "      <td>50085</td>\n",
       "      <td>500.0</td>\n",
       "      <td>1277</td>\n",
       "      <td>30.119439</td>\n",
       "      <td>-4.249420</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>520</td>\n",
       "      <td>1978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38160</th>\n",
       "      <td>72046</td>\n",
       "      <td>1500.0</td>\n",
       "      <td>2007</td>\n",
       "      <td>34.603101</td>\n",
       "      <td>-9.274893</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>30</td>\n",
       "      <td>1984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6050</th>\n",
       "      <td>64630</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>33.515113</td>\n",
       "      <td>-3.141018</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12578</th>\n",
       "      <td>69977</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>32.789723</td>\n",
       "      <td>-8.991740</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56315</th>\n",
       "      <td>15272</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-22</td>\n",
       "      <td>38.900205</td>\n",
       "      <td>-6.443328</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>800</td>\n",
       "      <td>2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48801</th>\n",
       "      <td>17499</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1588</td>\n",
       "      <td>34.929878</td>\n",
       "      <td>-4.418918</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>2</td>\n",
       "      <td>2992</td>\n",
       "      <td>2008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5891</th>\n",
       "      <td>53049</td>\n",
       "      <td>500.0</td>\n",
       "      <td>2145</td>\n",
       "      <td>36.658177</td>\n",
       "      <td>-3.222011</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>150</td>\n",
       "      <td>1990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59254</th>\n",
       "      <td>63798</td>\n",
       "      <td>0.0</td>\n",
       "      <td>677</td>\n",
       "      <td>37.710718</td>\n",
       "      <td>-9.719307</td>\n",
       "      <td>0</td>\n",
       "      <td>80</td>\n",
       "      <td>43</td>\n",
       "      <td>250</td>\n",
       "      <td>1992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3731</th>\n",
       "      <td>46173</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1237</td>\n",
       "      <td>34.311775</td>\n",
       "      <td>-1.657887</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "      <td>750</td>\n",
       "      <td>2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23046</th>\n",
       "      <td>69898</td>\n",
       "      <td>3000.0</td>\n",
       "      <td>1302</td>\n",
       "      <td>34.765801</td>\n",
       "      <td>-8.721545</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>120</td>\n",
       "      <td>1980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28292</th>\n",
       "      <td>4449</td>\n",
       "      <td>600.0</td>\n",
       "      <td>976</td>\n",
       "      <td>29.724980</td>\n",
       "      <td>-4.790095</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>500</td>\n",
       "      <td>2008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19682</th>\n",
       "      <td>15973</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>33.954588</td>\n",
       "      <td>-9.448471</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38481</th>\n",
       "      <td>850</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>33.589740</td>\n",
       "      <td>-8.930013</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7096</th>\n",
       "      <td>12689</td>\n",
       "      <td>0.0</td>\n",
       "      <td>995</td>\n",
       "      <td>35.483043</td>\n",
       "      <td>-10.625817</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36363</th>\n",
       "      <td>7803</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1227</td>\n",
       "      <td>37.638517</td>\n",
       "      <td>-3.240390</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48006</th>\n",
       "      <td>28831</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>32.908238</td>\n",
       "      <td>-2.591489</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9795</th>\n",
       "      <td>29534</td>\n",
       "      <td>50.0</td>\n",
       "      <td>489</td>\n",
       "      <td>38.268574</td>\n",
       "      <td>-5.450254</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>1300</td>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58170</th>\n",
       "      <td>20672</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>33.926294</td>\n",
       "      <td>-9.641293</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17191</th>\n",
       "      <td>10282</td>\n",
       "      <td>0.0</td>\n",
       "      <td>599</td>\n",
       "      <td>39.262924</td>\n",
       "      <td>-10.768079</td>\n",
       "      <td>0</td>\n",
       "      <td>90</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "      <td>2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8192</th>\n",
       "      <td>64193</td>\n",
       "      <td>30.0</td>\n",
       "      <td>426</td>\n",
       "      <td>39.348550</td>\n",
       "      <td>-10.642069</td>\n",
       "      <td>0</td>\n",
       "      <td>90</td>\n",
       "      <td>33</td>\n",
       "      <td>320</td>\n",
       "      <td>1988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49783</th>\n",
       "      <td>28687</td>\n",
       "      <td>50.0</td>\n",
       "      <td>501</td>\n",
       "      <td>37.562148</td>\n",
       "      <td>-6.888409</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>90</td>\n",
       "      <td>1982</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>47520 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  amount_tsh  gps_height  longitude   latitude  num_private  \\\n",
       "43360  72938         0.0           0  33.542898  -9.174777            0   \n",
       "7263   65358       500.0        2049  34.665760  -9.308548            0   \n",
       "2486     469        25.0         290  38.238568  -6.179919            0   \n",
       "313     1298         0.0           0  30.716727  -1.289055            0   \n",
       "52726  27001         0.0           0  35.389331  -6.399942            0   \n",
       "8558   41546         0.0        1295  31.214583  -8.431428            0   \n",
       "2559   16230     20000.0        1515  36.696700  -3.337926            0   \n",
       "54735  10307         0.0           0  36.292724  -5.177333            0   \n",
       "25763  37145         0.0           0  32.877248  -8.925921            0   \n",
       "44540  49234         0.0           0  33.014412  -3.115869            0   \n",
       "28603  52130         0.0         286  38.950769 -10.889023            0   \n",
       "4372   16650        20.0          17  39.020094  -5.346661            3   \n",
       "30666  36612         0.0        1404  36.694357  -3.035552            0   \n",
       "6431   19057        50.0         -12  38.890990  -6.444754            0   \n",
       "57420   4085         0.0           0  31.746292  -1.253936            0   \n",
       "1373   12040         0.0        1466  37.501573  -3.277989            0   \n",
       "2026   21481      6500.0        1969  35.371400  -4.470802            0   \n",
       "58977  69234         0.0         319  38.018784  -9.568769            0   \n",
       "41101  48982       500.0         884  29.660968  -4.818648            0   \n",
       "10019  53992        10.0        1472  35.714295  -3.314773            0   \n",
       "5103    3658         0.0        1274  30.241727  -4.182626            0   \n",
       "36712  45331        25.0        1551  31.023716  -7.490116            0   \n",
       "29670  72431         0.0         292  35.944307  -8.525588            0   \n",
       "54588  64881         0.0           0  32.580108  -3.549380            0   \n",
       "31867  10831       200.0         217  36.679869  -8.157754            0   \n",
       "12090  25701         0.0         312  38.975985 -10.528595            0   \n",
       "48300  13557        50.0         983  35.451872 -10.640641            0   \n",
       "16754    475      2200.0        1795  34.815906  -9.656551            0   \n",
       "50582  44263        20.0         716  35.889997  -7.148531            0   \n",
       "9136   50490      1000.0        1276  30.555693  -3.626554            0   \n",
       "...      ...         ...         ...        ...        ...          ...   \n",
       "26450  25540         5.0         976  37.245432  -3.340166            0   \n",
       "56373  56963         0.0           0  33.263250  -2.561022            0   \n",
       "4595   74226         0.0        1597  34.671798  -1.861973            0   \n",
       "35771  18487         0.0           0  33.813449  -9.120584            0   \n",
       "41564  22447      1000.0        1241  30.030766  -4.831457            0   \n",
       "30571  20930         0.0           0  31.678456  -2.406259            0   \n",
       "25961  57869         0.0        1325  34.543563  -5.059194            0   \n",
       "21280  28898         0.0        1251  35.014961  -5.017372            0   \n",
       "40122  38200      3000.0         979  31.074871  -7.094016            0   \n",
       "42270  50085       500.0        1277  30.119439  -4.249420            0   \n",
       "38160  72046      1500.0        2007  34.603101  -9.274893            0   \n",
       "6050   64630         0.0           0  33.515113  -3.141018            0   \n",
       "12578  69977         0.0           0  32.789723  -8.991740            0   \n",
       "56315  15272         0.0         -22  38.900205  -6.443328            0   \n",
       "48801  17499       100.0        1588  34.929878  -4.418918            0   \n",
       "5891   53049       500.0        2145  36.658177  -3.222011            0   \n",
       "59254  63798         0.0         677  37.710718  -9.719307            0   \n",
       "3731   46173         0.0        1237  34.311775  -1.657887            0   \n",
       "23046  69898      3000.0        1302  34.765801  -8.721545            0   \n",
       "28292   4449       600.0         976  29.724980  -4.790095            0   \n",
       "19682  15973         0.0           0  33.954588  -9.448471            0   \n",
       "38481    850         0.0           0  33.589740  -8.930013            0   \n",
       "7096   12689         0.0         995  35.483043 -10.625817            0   \n",
       "36363   7803         0.0        1227  37.638517  -3.240390            0   \n",
       "48006  28831         0.0           0  32.908238  -2.591489            0   \n",
       "9795   29534        50.0         489  38.268574  -5.450254            4   \n",
       "58170  20672         0.0           0  33.926294  -9.641293            0   \n",
       "17191  10282         0.0         599  39.262924 -10.768079            0   \n",
       "8192   64193        30.0         426  39.348550 -10.642069            0   \n",
       "49783  28687        50.0         501  37.562148  -6.888409            0   \n",
       "\n",
       "       region_code  district_code  population  construction_year  \n",
       "43360           12              4           0                  0  \n",
       "7263            11              4         175               2008  \n",
       "2486             6              1        2300               2010  \n",
       "313             18              1           0                  0  \n",
       "52726            1              6           0                  0  \n",
       "8558            15              2         200               1986  \n",
       "2559             2              2         150               1995  \n",
       "54735            1              1           0                  0  \n",
       "25763           12              6           0                  0  \n",
       "44540           19              7           0                  0  \n",
       "28603           90             33           1               1985  \n",
       "4372             4              5         300               2009  \n",
       "30666            2              6         200               2001  \n",
       "6431             6              1          30               2008  \n",
       "57420           18              7           0                  0  \n",
       "1373             3              4          15               1972  \n",
       "2026            21              2         162               2003  \n",
       "58977           80             43         250               2006  \n",
       "41101           16              3        1500               1985  \n",
       "10019           24             30         150               1994  \n",
       "5103            16              2        4210               1996  \n",
       "36712           15              3           1               1995  \n",
       "29670            5              3         500               1980  \n",
       "54588           17              3           0                  0  \n",
       "31867            5              3         280               1979  \n",
       "12090           90             33          30               2010  \n",
       "48300           10              2           0               2009  \n",
       "16754           11              4          40               2005  \n",
       "50582           11              1           1               2006  \n",
       "9136            16              1         100               2005  \n",
       "...            ...            ...         ...                ...  \n",
       "26450            3              5          50               1999  \n",
       "56373           19              2           0                  0  \n",
       "4595            20              2         400               1994  \n",
       "35771           12              4           0                  0  \n",
       "41564           16              2         750               1984  \n",
       "30571           18              8           0                  0  \n",
       "25961           13              2           1               2003  \n",
       "21280           13              2         230               2000  \n",
       "40122           15              1         500               2006  \n",
       "42270           16              2         520               1978  \n",
       "38160           11              4          30               1984  \n",
       "6050            17              2           0                  0  \n",
       "12578           12              6           0                  0  \n",
       "56315            6              1         800               2007  \n",
       "48801           21              2        2992               2008  \n",
       "5891             2              2         150               1990  \n",
       "59254           80             43         250               1992  \n",
       "3731            20              2         750               2011  \n",
       "23046           11              4         120               1980  \n",
       "28292           16              3         500               2008  \n",
       "19682           12              3           0                  0  \n",
       "38481           12              2           0                  0  \n",
       "7096            10              2           0               2003  \n",
       "36363            3              1           1               2003  \n",
       "48006           19              3           0                  0  \n",
       "9795             4              6        1300               2009  \n",
       "58170           12              3           0                  0  \n",
       "17191           90             33           1               2012  \n",
       "8192            90             33         320               1988  \n",
       "49783            5              6          90               1982  \n",
       "\n",
       "[47520 rows x 10 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_numeric = X_train.select_dtypes('number')\n",
    "X_train_numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_numeric = X_val.select_dtypes('number')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5cVaFgL_8lZl"
   },
   "source": [
    "#### Drop nulls if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FAkDFto77qec"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                   0\n",
       "amount_tsh           0\n",
       "gps_height           0\n",
       "longitude            0\n",
       "latitude             0\n",
       "num_private          0\n",
       "region_code          0\n",
       "district_code        0\n",
       "population           0\n",
       "construction_year    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_numeric.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xMJL579p8tSM"
   },
   "source": [
    "#### Fit Logistic Regresson on train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2pEyqCGy7-kZ"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.21.3'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn\n",
    "sklearn.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\conno\\Anaconda3\\envs\\unit2\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "C:\\Users\\conno\\Anaconda3\\envs\\unit2\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "C:\\Users\\conno\\Anaconda3\\envs\\unit2\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\conno\\Anaconda3\\envs\\unit2\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\conno\\Anaconda3\\envs\\unit2\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\conno\\Anaconda3\\envs\\unit2\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\conno\\Anaconda3\\envs\\unit2\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\conno\\Anaconda3\\envs\\unit2\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegressionCV(Cs=10, class_weight=None, cv='warn', dual=False,\n",
       "                     fit_intercept=True, intercept_scaling=1.0, l1_ratios=None,\n",
       "                     max_iter=100, multi_class='warn', n_jobs=None,\n",
       "                     penalty='l2', random_state=None, refit=True, scoring=None,\n",
       "                     solver='lbfgs', tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "# Instantiate\n",
    "model = LogisticRegressionCV()\n",
    "# Fit\n",
    "model.fit(X_train_numeric, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WyIUh-th9Bnw"
   },
   "source": [
    "#### Evaluate on validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Um_q4k9-8zvp"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.553956228956229"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "y_pred = model.predict(X_val_numeric)\n",
    "accuracy_score(y_val,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.553956228956229"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(X_val_numeric,y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jgYwtN7D9ewk"
   },
   "source": [
    "#### What predictions does a Logistic Regression return?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-X9KwbEl9VJu"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['functional', 'functional', 'functional', ..., 'functional',\n",
       "       'functional', 'non functional'], dtype=object)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "functional        10779\n",
       "non functional     1101\n",
       "dtype: int64"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(y_pred).value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.50236571, 0.06959266, 0.42804162],\n",
       "       [0.64842847, 0.10114691, 0.25042462],\n",
       "       [0.55571509, 0.07703544, 0.36724947],\n",
       "       ...,\n",
       "       [0.49016887, 0.08605103, 0.42378011],\n",
       "       [0.63343391, 0.06560989, 0.3009562 ],\n",
       "       [0.3028229 , 0.04976926, 0.64740783]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_proba = model.predict_proba(X_val_numeric)\n",
    "y_pred_proba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CkE2lbblr7Fn"
   },
   "source": [
    "## Do one-hot encoding of categorical features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y1AuoNR-BO-N"
   },
   "source": [
    "### Check \"cardinality\" of categorical features\n",
    "\n",
    "[Cardinality](https://simple.wikipedia.org/wiki/Cardinality) means the number of unique values that a feature has:\n",
    "> In mathematics, the cardinality of a set means the number of its elements. For example, the set A = {2, 4, 6} contains 3 elements, and therefore A has a cardinality of 3. \n",
    "\n",
    "One-hot encoding adds a dimension for each unique value of each categorical feature. So, it may not be a good choice for \"high cardinality\" categoricals that have dozens, hundreds, or thousands of unique values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hLbD2DLmAm1g"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>unique</th>\n",
       "      <th>top</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>recorded_by</th>\n",
       "      <td>47520</td>\n",
       "      <td>1</td>\n",
       "      <td>GeoData Consultants Ltd</td>\n",
       "      <td>47520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>public_meeting</th>\n",
       "      <td>44876</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>40838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>permit</th>\n",
       "      <td>45077</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>31071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>source_class</th>\n",
       "      <td>47520</td>\n",
       "      <td>3</td>\n",
       "      <td>groundwater</td>\n",
       "      <td>36638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>management_group</th>\n",
       "      <td>47520</td>\n",
       "      <td>5</td>\n",
       "      <td>user-group</td>\n",
       "      <td>42027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quantity_group</th>\n",
       "      <td>47520</td>\n",
       "      <td>5</td>\n",
       "      <td>enough</td>\n",
       "      <td>26567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quantity</th>\n",
       "      <td>47520</td>\n",
       "      <td>5</td>\n",
       "      <td>enough</td>\n",
       "      <td>26567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>waterpoint_type_group</th>\n",
       "      <td>47520</td>\n",
       "      <td>6</td>\n",
       "      <td>communal standpipe</td>\n",
       "      <td>27642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quality_group</th>\n",
       "      <td>47520</td>\n",
       "      <td>6</td>\n",
       "      <td>good</td>\n",
       "      <td>40598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>payment_type</th>\n",
       "      <td>47520</td>\n",
       "      <td>7</td>\n",
       "      <td>never pay</td>\n",
       "      <td>20287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>source_type</th>\n",
       "      <td>47520</td>\n",
       "      <td>7</td>\n",
       "      <td>spring</td>\n",
       "      <td>13620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>waterpoint_type</th>\n",
       "      <td>47520</td>\n",
       "      <td>7</td>\n",
       "      <td>communal standpipe</td>\n",
       "      <td>22778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>extraction_type_class</th>\n",
       "      <td>47520</td>\n",
       "      <td>7</td>\n",
       "      <td>gravity</td>\n",
       "      <td>21448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>payment</th>\n",
       "      <td>47520</td>\n",
       "      <td>7</td>\n",
       "      <td>never pay</td>\n",
       "      <td>20287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>water_quality</th>\n",
       "      <td>47520</td>\n",
       "      <td>8</td>\n",
       "      <td>soft</td>\n",
       "      <td>40598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>basin</th>\n",
       "      <td>47520</td>\n",
       "      <td>9</td>\n",
       "      <td>Lake Victoria</td>\n",
       "      <td>8137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>source</th>\n",
       "      <td>47520</td>\n",
       "      <td>10</td>\n",
       "      <td>spring</td>\n",
       "      <td>13620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scheme_management</th>\n",
       "      <td>44392</td>\n",
       "      <td>12</td>\n",
       "      <td>VWC</td>\n",
       "      <td>29470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>management</th>\n",
       "      <td>47520</td>\n",
       "      <td>12</td>\n",
       "      <td>vwc</td>\n",
       "      <td>32449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>extraction_type_group</th>\n",
       "      <td>47520</td>\n",
       "      <td>13</td>\n",
       "      <td>gravity</td>\n",
       "      <td>21448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>extraction_type</th>\n",
       "      <td>47520</td>\n",
       "      <td>18</td>\n",
       "      <td>gravity</td>\n",
       "      <td>21448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>region</th>\n",
       "      <td>47520</td>\n",
       "      <td>21</td>\n",
       "      <td>Iringa</td>\n",
       "      <td>4250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lga</th>\n",
       "      <td>47520</td>\n",
       "      <td>124</td>\n",
       "      <td>Njombe</td>\n",
       "      <td>2003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date_recorded</th>\n",
       "      <td>47520</td>\n",
       "      <td>349</td>\n",
       "      <td>2011-03-17</td>\n",
       "      <td>474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>funder</th>\n",
       "      <td>44616</td>\n",
       "      <td>1716</td>\n",
       "      <td>Government Of Tanzania</td>\n",
       "      <td>7321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>installer</th>\n",
       "      <td>44603</td>\n",
       "      <td>1929</td>\n",
       "      <td>DWE</td>\n",
       "      <td>13978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ward</th>\n",
       "      <td>47520</td>\n",
       "      <td>2082</td>\n",
       "      <td>Igosi</td>\n",
       "      <td>257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scheme_name</th>\n",
       "      <td>24988</td>\n",
       "      <td>2563</td>\n",
       "      <td>K</td>\n",
       "      <td>548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subvillage</th>\n",
       "      <td>47234</td>\n",
       "      <td>17231</td>\n",
       "      <td>Shuleni</td>\n",
       "      <td>420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wpt_name</th>\n",
       "      <td>47520</td>\n",
       "      <td>30661</td>\n",
       "      <td>none</td>\n",
       "      <td>2879</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       count unique                      top   freq\n",
       "recorded_by            47520      1  GeoData Consultants Ltd  47520\n",
       "public_meeting         44876      2                     True  40838\n",
       "permit                 45077      2                     True  31071\n",
       "source_class           47520      3              groundwater  36638\n",
       "management_group       47520      5               user-group  42027\n",
       "quantity_group         47520      5                   enough  26567\n",
       "quantity               47520      5                   enough  26567\n",
       "waterpoint_type_group  47520      6       communal standpipe  27642\n",
       "quality_group          47520      6                     good  40598\n",
       "payment_type           47520      7                never pay  20287\n",
       "source_type            47520      7                   spring  13620\n",
       "waterpoint_type        47520      7       communal standpipe  22778\n",
       "extraction_type_class  47520      7                  gravity  21448\n",
       "payment                47520      7                never pay  20287\n",
       "water_quality          47520      8                     soft  40598\n",
       "basin                  47520      9            Lake Victoria   8137\n",
       "source                 47520     10                   spring  13620\n",
       "scheme_management      44392     12                      VWC  29470\n",
       "management             47520     12                      vwc  32449\n",
       "extraction_type_group  47520     13                  gravity  21448\n",
       "extraction_type        47520     18                  gravity  21448\n",
       "region                 47520     21                   Iringa   4250\n",
       "lga                    47520    124                   Njombe   2003\n",
       "date_recorded          47520    349               2011-03-17    474\n",
       "funder                 44616   1716   Government Of Tanzania   7321\n",
       "installer              44603   1929                      DWE  13978\n",
       "ward                   47520   2082                    Igosi    257\n",
       "scheme_name            24988   2563                        K    548\n",
       "subvillage             47234  17231                  Shuleni    420\n",
       "wpt_name               47520  30661                     none   2879"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.describe(exclude = 'number').T.sort_values(by = 'unique')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MbV7HjibCYV5"
   },
   "source": [
    "### Explore `quantity` feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iOZ3QQoFBhoS"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "enough          0.559070\n",
       "insufficient    0.255745\n",
       "dry             0.103556\n",
       "seasonal        0.068266\n",
       "unknown         0.013363\n",
       "Name: quantity, dtype: float64"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train['quantity'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "quantity      status_group           \n",
       "dry           non functional             0.967689\n",
       "              functional                 0.026011\n",
       "              functional needs repair    0.006300\n",
       "enough        functional                 0.650920\n",
       "              non functional             0.276998\n",
       "              functional needs repair    0.072082\n",
       "insufficient  functional                 0.521106\n",
       "              non functional             0.382786\n",
       "              functional needs repair    0.096108\n",
       "seasonal      functional                 0.580456\n",
       "              non functional             0.318434\n",
       "              functional needs repair    0.101110\n",
       "unknown       non functional             0.713386\n",
       "              functional                 0.267717\n",
       "              functional needs repair    0.018898\n",
       "Name: status_group, dtype: float64"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Recombine X_train and y_train, for exploratory data analysis \n",
    "train = X_train.copy()\n",
    "train['status_group'] = y_train\n",
    "\n",
    "#Now we do groupby\n",
    "train.groupby('quantity')['status_group'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>status_group</th>\n",
       "      <th>functional</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>43360</th>\n",
       "      <td>functional</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7263</th>\n",
       "      <td>functional</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2486</th>\n",
       "      <td>functional</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>313</th>\n",
       "      <td>non functional</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52726</th>\n",
       "      <td>functional</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8558</th>\n",
       "      <td>functional</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2559</th>\n",
       "      <td>functional</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54735</th>\n",
       "      <td>functional</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25763</th>\n",
       "      <td>non functional</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44540</th>\n",
       "      <td>non functional</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28603</th>\n",
       "      <td>non functional</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4372</th>\n",
       "      <td>non functional</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30666</th>\n",
       "      <td>functional</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6431</th>\n",
       "      <td>functional</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57420</th>\n",
       "      <td>non functional</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1373</th>\n",
       "      <td>functional</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2026</th>\n",
       "      <td>functional</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58977</th>\n",
       "      <td>non functional</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41101</th>\n",
       "      <td>non functional</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10019</th>\n",
       "      <td>functional</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5103</th>\n",
       "      <td>functional</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36712</th>\n",
       "      <td>functional</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29670</th>\n",
       "      <td>functional</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54588</th>\n",
       "      <td>functional</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31867</th>\n",
       "      <td>functional</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12090</th>\n",
       "      <td>functional</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48300</th>\n",
       "      <td>functional</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16754</th>\n",
       "      <td>functional</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50582</th>\n",
       "      <td>non functional</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9136</th>\n",
       "      <td>non functional</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26450</th>\n",
       "      <td>functional</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56373</th>\n",
       "      <td>non functional</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4595</th>\n",
       "      <td>functional</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35771</th>\n",
       "      <td>functional needs repair</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41564</th>\n",
       "      <td>functional</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30571</th>\n",
       "      <td>functional</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25961</th>\n",
       "      <td>non functional</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21280</th>\n",
       "      <td>functional</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40122</th>\n",
       "      <td>functional</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42270</th>\n",
       "      <td>non functional</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38160</th>\n",
       "      <td>functional</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6050</th>\n",
       "      <td>non functional</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12578</th>\n",
       "      <td>functional</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56315</th>\n",
       "      <td>functional</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48801</th>\n",
       "      <td>functional</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5891</th>\n",
       "      <td>non functional</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59254</th>\n",
       "      <td>functional</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3731</th>\n",
       "      <td>functional</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23046</th>\n",
       "      <td>functional</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28292</th>\n",
       "      <td>functional needs repair</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19682</th>\n",
       "      <td>functional</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38481</th>\n",
       "      <td>functional</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7096</th>\n",
       "      <td>functional</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36363</th>\n",
       "      <td>functional</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48006</th>\n",
       "      <td>functional</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9795</th>\n",
       "      <td>non functional</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58170</th>\n",
       "      <td>non functional</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17191</th>\n",
       "      <td>non functional</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8192</th>\n",
       "      <td>functional</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49783</th>\n",
       "      <td>functional</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>47520 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  status_group  functional\n",
       "43360               functional           1\n",
       "7263                functional           1\n",
       "2486                functional           1\n",
       "313             non functional           0\n",
       "52726               functional           1\n",
       "8558                functional           1\n",
       "2559                functional           1\n",
       "54735               functional           1\n",
       "25763           non functional           0\n",
       "44540           non functional           0\n",
       "28603           non functional           0\n",
       "4372            non functional           0\n",
       "30666               functional           1\n",
       "6431                functional           1\n",
       "57420           non functional           0\n",
       "1373                functional           1\n",
       "2026                functional           1\n",
       "58977           non functional           0\n",
       "41101           non functional           0\n",
       "10019               functional           1\n",
       "5103                functional           1\n",
       "36712               functional           1\n",
       "29670               functional           1\n",
       "54588               functional           1\n",
       "31867               functional           1\n",
       "12090               functional           1\n",
       "48300               functional           1\n",
       "16754               functional           1\n",
       "50582           non functional           0\n",
       "9136            non functional           0\n",
       "...                        ...         ...\n",
       "26450               functional           1\n",
       "56373           non functional           0\n",
       "4595                functional           1\n",
       "35771  functional needs repair           0\n",
       "41564               functional           1\n",
       "30571               functional           1\n",
       "25961           non functional           0\n",
       "21280               functional           1\n",
       "40122               functional           1\n",
       "42270           non functional           0\n",
       "38160               functional           1\n",
       "6050            non functional           0\n",
       "12578               functional           1\n",
       "56315               functional           1\n",
       "48801               functional           1\n",
       "5891            non functional           0\n",
       "59254               functional           1\n",
       "3731                functional           1\n",
       "23046               functional           1\n",
       "28292  functional needs repair           0\n",
       "19682               functional           1\n",
       "38481               functional           1\n",
       "7096                functional           1\n",
       "36363               functional           1\n",
       "48006               functional           1\n",
       "9795            non functional           0\n",
       "58170           non functional           0\n",
       "17191           non functional           0\n",
       "8192                functional           1\n",
       "49783               functional           1\n",
       "\n",
       "[47520 rows x 2 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "train['functional'] = (train['status_group']== 'functional').astype(int)\n",
    "train[['status_group', 'functional']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1, 'Percentage of Waterpumps Functional By Water Quality')"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.catplot(x='quantity', y='functional', data = train, kind = 'bar', color = 'grey')\n",
    "plt.title('Percentage of Waterpumps Functional By Water Quality')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6kCA47KPr9PE"
   },
   "source": [
    "## Do one-hot encoding & Scale features, \n",
    "within a complete model fitting workflow.\n",
    "\n",
    "### Why and how to scale features before fitting linear models\n",
    "\n",
    "Scikit-Learn User Guide, [Preprocessing data](https://scikit-learn.org/stable/modules/preprocessing.html)\n",
    "> Standardization of datasets is a common requirement for many machine learning estimators implemented in scikit-learn; they might behave badly if the individual features do not more or less look like standard normally distributed data: Gaussian with zero mean and unit variance.\n",
    "\n",
    "> The `preprocessing` module further provides a utility class `StandardScaler` that implements the `Transformer` API to compute the mean and standard deviation on a training set. The scaler instance can then be used on new data to transform it the same way it did on the training set.\n",
    "\n",
    "### How to use encoders and scalers in scikit-learn\n",
    "- Use the **`fit_transform`** method on the **train** set\n",
    "- Use the **`transform`** method on the **validation** set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yTkS24UwHJHa"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\conno\\Anaconda3\\envs\\unit2\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "C:\\Users\\conno\\Anaconda3\\envs\\unit2\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy 0.6585016835016835\n"
     ]
    }
   ],
   "source": [
    "import category_encoders as ce \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "categorical_features = ['quantity']\n",
    "numeric_features = X_train.select_dtypes('number').columns.drop('id').tolist()\n",
    "features = categorical_features + numeric_features\n",
    "\n",
    "X_train_subset = X_train[features]\n",
    "X_val_subset = X_val[features]\n",
    "\n",
    "encoder = ce.OneHotEncoder(use_cat_names=True)\n",
    "X_train_encoded = encoder.fit_transform(X_train_subset)\n",
    "X_val_encoded = encoder.transform(X_val_subset)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_encoded)\n",
    "X_val_scaled = scaler.transform(X_val_encoded)\n",
    "\n",
    "model = LogisticRegressionCV()\n",
    "model.fit(X_train_scaled, y_train)\n",
    "print('Validation Accuracy', model.score(X_val_scaled, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Chix-W9-LTEX"
   },
   "source": [
    "### Compare original features, encoded features, & scaled features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YhJ3PHTAKzFx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(47520, 40)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>amount_tsh</th>\n",
       "      <th>date_recorded</th>\n",
       "      <th>funder</th>\n",
       "      <th>gps_height</th>\n",
       "      <th>installer</th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>wpt_name</th>\n",
       "      <th>num_private</th>\n",
       "      <th>...</th>\n",
       "      <th>payment_type</th>\n",
       "      <th>water_quality</th>\n",
       "      <th>quality_group</th>\n",
       "      <th>quantity</th>\n",
       "      <th>quantity_group</th>\n",
       "      <th>source</th>\n",
       "      <th>source_type</th>\n",
       "      <th>source_class</th>\n",
       "      <th>waterpoint_type</th>\n",
       "      <th>waterpoint_type_group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>43360</th>\n",
       "      <td>72938</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2011-07-27</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>33.542898</td>\n",
       "      <td>-9.174777</td>\n",
       "      <td>Kwa Mzee Noa</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>never pay</td>\n",
       "      <td>soft</td>\n",
       "      <td>good</td>\n",
       "      <td>insufficient</td>\n",
       "      <td>insufficient</td>\n",
       "      <td>spring</td>\n",
       "      <td>spring</td>\n",
       "      <td>groundwater</td>\n",
       "      <td>communal standpipe</td>\n",
       "      <td>communal standpipe</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  amount_tsh date_recorded funder  gps_height installer  \\\n",
       "43360  72938         0.0    2011-07-27    NaN           0       NaN   \n",
       "\n",
       "       longitude  latitude      wpt_name  num_private          ...           \\\n",
       "43360  33.542898 -9.174777  Kwa Mzee Noa            0          ...            \n",
       "\n",
       "      payment_type water_quality quality_group      quantity  quantity_group  \\\n",
       "43360    never pay          soft          good  insufficient    insufficient   \n",
       "\n",
       "       source source_type  source_class     waterpoint_type  \\\n",
       "43360  spring      spring   groundwater  communal standpipe   \n",
       "\n",
       "      waterpoint_type_group  \n",
       "43360    communal standpipe  \n",
       "\n",
       "[1 rows x 40 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shape of the original frame\n",
    "print(X_train.shape)\n",
    "X_train[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(47520, 10)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>quantity</th>\n",
       "      <th>amount_tsh</th>\n",
       "      <th>gps_height</th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>num_private</th>\n",
       "      <th>region_code</th>\n",
       "      <th>district_code</th>\n",
       "      <th>population</th>\n",
       "      <th>construction_year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>43360</th>\n",
       "      <td>insufficient</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>33.542898</td>\n",
       "      <td>-9.174777</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           quantity  amount_tsh  gps_height  longitude  latitude  num_private  \\\n",
       "43360  insufficient         0.0           0  33.542898 -9.174777            0   \n",
       "\n",
       "       region_code  district_code  population  construction_year  \n",
       "43360           12              4           0                  0  "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shape of subset frame\n",
    "print(X_train_subset.shape)\n",
    "X_train_subset[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(47520, 14)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>quantity_insufficient</th>\n",
       "      <th>quantity_enough</th>\n",
       "      <th>quantity_dry</th>\n",
       "      <th>quantity_seasonal</th>\n",
       "      <th>quantity_unknown</th>\n",
       "      <th>amount_tsh</th>\n",
       "      <th>gps_height</th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>num_private</th>\n",
       "      <th>region_code</th>\n",
       "      <th>district_code</th>\n",
       "      <th>population</th>\n",
       "      <th>construction_year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>43360</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>33.542898</td>\n",
       "      <td>-9.174777</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       quantity_insufficient  quantity_enough  quantity_dry  \\\n",
       "43360                      1                0             0   \n",
       "\n",
       "       quantity_seasonal  quantity_unknown  amount_tsh  gps_height  longitude  \\\n",
       "43360                  0                 0         0.0           0  33.542898   \n",
       "\n",
       "       latitude  num_private  region_code  district_code  population  \\\n",
       "43360 -9.174777            0           12              4           0   \n",
       "\n",
       "       construction_year  \n",
       "43360                  0  "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shape of encoded dataframe\n",
    "print(X_train_encoded.shape)\n",
    "X_train_encoded[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZfVECpN7J6gb"
   },
   "source": [
    "### Get & plot coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9nHkKk5XKwVm"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.34072363e-01,  4.10951619e-01, -9.47084405e-01,\n",
       "         1.40772178e-01, -8.24851579e-02,  3.53781263e-01,\n",
       "         2.15674988e-01,  4.48213423e-04, -1.30243712e-01,\n",
       "         2.95020953e-03, -2.52559298e-01,  1.10785996e-01,\n",
       "         2.61979488e-02, -1.27220926e-01],\n",
       "       [ 4.58527547e-02,  6.66605840e-04, -8.08056088e-02,\n",
       "         2.73808754e-02, -2.28254790e-02, -2.64965320e-03,\n",
       "        -9.57584988e-03, -9.86022379e-02,  2.84648022e-02,\n",
       "        -3.34295544e-03,  1.19487147e-02, -1.70460881e-02,\n",
       "         1.59968060e-03, -2.36011014e-02],\n",
       "       [-1.64130501e-01, -4.11475105e-01,  1.00385326e+00,\n",
       "        -1.69002588e-01,  1.10326674e-01, -4.70101009e-01,\n",
       "        -2.29590761e-01,  1.68839949e-01,  1.64522504e-01,\n",
       "        -1.01437135e-03,  2.25715395e-01, -8.91399128e-02,\n",
       "        -2.96384544e-02,  1.30939581e-01]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x25ef053db88>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "functional_coefficients = pd.Series(\n",
    "    model.coef_[0],\n",
    "    X_train_encoded.columns\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "functional_coefficients.sort_values().plot.barh()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZhUzucgPr_he"
   },
   "source": [
    "## Submit to predictive modeling competition\n",
    "\n",
    "\n",
    "### Write submission CSV file\n",
    "\n",
    "The format for the submission file is simply the row id and the predicted label (for an example, see `sample_submission.csv` on the data download page.\n",
    "\n",
    "For example, if you just predicted that all the waterpoints were functional you would have the following predictions:\n",
    "\n",
    "<pre>id,status_group\n",
    "50785,functional\n",
    "51630,functional\n",
    "17168,functional\n",
    "45559,functional\n",
    "49871,functional\n",
    "</pre>\n",
    "\n",
    "Your code to generate a submission file may look like this: \n",
    "<pre># estimator is your scikit-learn estimator, which you've fit on X_train\n",
    "\n",
    "# X_test is your pandas dataframe or numpy array, \n",
    "# with the same number of rows, in the same order, as test_features.csv, \n",
    "# and the same number of columns, in the same order, as X_train\n",
    "\n",
    "y_pred = estimator.predict(X_test)\n",
    "\n",
    "\n",
    "# Makes a dataframe with two columns, id and status_group, \n",
    "# and writes to a csv file, without the index\n",
    "\n",
    "sample_submission = pd.read_csv('sample_submission.csv')\n",
    "submission = sample_submission.copy()\n",
    "submission['status_group'] = y_pred\n",
    "submission.to_csv('your-submission-filename.csv', index=False)\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_subset = test_features[features]\n",
    "X_test_encoded = encoder.transform(X_test_subset)\n",
    "X_test_scaled = scaler.transform(X_test_encoded)\n",
    "assert all(X_test_encoded.columns == X_train_encoded.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yRitgZ_ULx6K"
   },
   "outputs": [],
   "source": [
    "submission = sample_submission.copy()\n",
    "submission['status_group'] = y_pred\n",
    "submission.to_csv('submission-01.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PpG9knom1FN7"
   },
   "source": [
    "### Send submission CSV file to Kaggle\n",
    "\n",
    "#### Option 1. Kaggle web UI\n",
    " \n",
    "Go to our Kaggle InClass competition webpage. Use the blue **Submit Predictions** button to upload your CSV file.\n",
    "\n",
    "\n",
    "#### Option 2. Kaggle API\n",
    "\n",
    "Use the Kaggle API to upload your CSV file."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "lesson_regression_classification_4.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
