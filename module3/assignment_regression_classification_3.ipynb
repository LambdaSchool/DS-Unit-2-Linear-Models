{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "assignment_regression_classification_3.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hurshd0/DS-Unit-2-Regression-Classification/blob/master/module3/assignment_regression_classification_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7IXUfiQ2UKj6",
        "colab_type": "text"
      },
      "source": [
        "Lambda School Data Science, Unit 2: Predictive Modeling\n",
        "\n",
        "# Regression & Classification, Module 3\n",
        "\n",
        "## Assignment\n",
        "\n",
        "We're going back to our other **New York City** real estate dataset. Instead of predicting apartment rents, you'll predict property sales prices.\n",
        "\n",
        "But not just for condos in Tribeca...\n",
        "\n",
        "Instead, predict property sales prices for **One Family Dwellings** (`BUILDING_CLASS_CATEGORY` == `'01 ONE FAMILY DWELLINGS'`) using a subset of the data where the **sale price was more than \\\\$100 thousand and less than $2 million.** \n",
        "\n",
        "The [NYC Department of Finance](https://www1.nyc.gov/site/finance/taxes/property-rolling-sales-data.page) has a glossary of property sales terms and NYC Building Class Code Descriptions. The data comes from the [NYC OpenData](https://data.cityofnewyork.us/browse?q=NYC%20calendar%20sales) portal.\n",
        "\n",
        "\n",
        "- [X] Do train/test split. Use data from January — March 2019 to train. Use data from April 2019 to test.\n",
        "- [X] Do exploratory visualizations with Seaborn.\n",
        "- [X] Do one-hot encoding of categorical features.\n",
        "- [X] Do feature selection with `SelectKBest`.\n",
        "- [X] Fit a linear regression model with multiple features.\n",
        "- [X] Get mean absolute error for the test set.\n",
        "- [X] As always, commit your notebook to your fork of the GitHub repo.\n",
        "\n",
        "\n",
        "## Stretch Goals\n",
        "- [X] Add your own stretch goal(s) !\n",
        "- [X] Do [feature scaling](https://scikit-learn.org/stable/modules/preprocessing.html).\n",
        "- [ ] Learn more about feature selection:\n",
        "    - [\"Permutation importance\"](https://www.kaggle.com/dansbecker/permutation-importance)\n",
        "    - [scikit-learn's User Guide for Feature Selection](https://scikit-learn.org/stable/modules/feature_selection.html)\n",
        "    - [mlxtend](http://rasbt.github.io/mlxtend/) library\n",
        "    - scikit-learn-contrib libraries: [boruta_py](https://github.com/scikit-learn-contrib/boruta_py) & [stability-selection](https://github.com/scikit-learn-contrib/stability-selection)\n",
        "    - [_Feature Engineering and Selection_](http://www.feat.engineering/) by Kuhn & Johnson.\n",
        "- [ ] Try [statsmodels](https://www.statsmodels.org/stable/index.html) if you’re interested in more inferential statistical approach to linear regression and feature selection, looking at p values and 95% confidence intervals for the coefficients.\n",
        "- [ ] Read [_An Introduction to Statistical Learning_](http://faculty.marshall.usc.edu/gareth-james/ISL/ISLR%20Seventh%20Printing.pdf), Chapters 1-3, for more math & theory, but in an accessible, readable way (without an excessive amount of formulas or academic pre-requisites).\n",
        "(That book is good regardless of whether your cultural worldview is inferential statistics or predictive machine learning)\n",
        "- [ ] Read Leo Breiman's paper, [\"Statistical Modeling: The Two Cultures\"](https://projecteuclid.org/download/pdf_1/euclid.ss/1009213726)\n",
        "- [ ] Try [scikit-learn pipelines](https://scikit-learn.org/stable/modules/compose.html):\n",
        "\n",
        "> Pipeline can be used to chain multiple estimators into one. This is useful as there is often a fixed sequence of steps in processing the data, for example feature selection, normalization and classification. Pipeline serves multiple purposes here:\n",
        "\n",
        "> - **Convenience and encapsulation.** You only have to call fit and predict once on your data to fit a whole sequence of estimators.\n",
        "> - **Joint parameter selection.** You can grid search over parameters of all estimators in the pipeline at once.\n",
        "> - **Safety.** Pipelines help avoid leaking statistics from your test data into the trained model in cross-validation, by ensuring that the same samples are used to train the transformers and predictors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o9eSnDYhUGD7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d2c44665-44e0-409f-c3f7-1cf1652584e3"
      },
      "source": [
        "# If you're in Colab...\n",
        "import os, sys\n",
        "in_colab = 'google.colab' in sys.modules\n",
        "\n",
        "if in_colab:\n",
        "    # Install required python packages:\n",
        "    # category_encoders, version >= 2.0\n",
        "    # pandas-profiling, version >= 2.0\n",
        "    # plotly, version >= 4.0\n",
        "    !pip install --upgrade category_encoders pandas-profiling plotly\n",
        "    \n",
        "    # Pull files from Github repo\n",
        "    os.chdir('/content')\n",
        "    !git init .\n",
        "    !git remote add origin https://github.com/hurshd0/DS-Unit-2-Regression-Classification.git\n",
        "    !git pull origin master\n",
        "    \n",
        "    # Change into directory for module\n",
        "    os.chdir('module3')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: category_encoders in /usr/local/lib/python3.6/dist-packages (2.0.0)\n",
            "Requirement already up-to-date: pandas-profiling in /usr/local/lib/python3.6/dist-packages (2.3.0)\n",
            "Requirement already up-to-date: plotly in /usr/local/lib/python3.6/dist-packages (4.1.0)\n",
            "Requirement already satisfied, skipping upgrade: scipy>=0.19.0 in /usr/local/lib/python3.6/dist-packages (from category_encoders) (1.3.0)\n",
            "Requirement already satisfied, skipping upgrade: statsmodels>=0.6.1 in /usr/local/lib/python3.6/dist-packages (from category_encoders) (0.10.1)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from category_encoders) (1.16.4)\n",
            "Requirement already satisfied, skipping upgrade: scikit-learn>=0.20.0 in /usr/local/lib/python3.6/dist-packages (from category_encoders) (0.21.3)\n",
            "Requirement already satisfied, skipping upgrade: patsy>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from category_encoders) (0.5.1)\n",
            "Requirement already satisfied, skipping upgrade: pandas>=0.21.1 in /usr/local/lib/python3.6/dist-packages (from category_encoders) (0.24.2)\n",
            "Requirement already satisfied, skipping upgrade: htmlmin>=0.1.12 in /usr/local/lib/python3.6/dist-packages (from pandas-profiling) (0.1.12)\n",
            "Requirement already satisfied, skipping upgrade: astropy in /usr/local/lib/python3.6/dist-packages (from pandas-profiling) (3.0.5)\n",
            "Requirement already satisfied, skipping upgrade: jinja2>=2.8 in /usr/local/lib/python3.6/dist-packages (from pandas-profiling) (2.10.1)\n",
            "Requirement already satisfied, skipping upgrade: phik>=0.9.8 in /usr/local/lib/python3.6/dist-packages (from pandas-profiling) (0.9.8)\n",
            "Requirement already satisfied, skipping upgrade: missingno>=0.4.2 in /usr/local/lib/python3.6/dist-packages (from pandas-profiling) (0.4.2)\n",
            "Requirement already satisfied, skipping upgrade: matplotlib>=1.4 in /usr/local/lib/python3.6/dist-packages (from pandas-profiling) (3.0.3)\n",
            "Requirement already satisfied, skipping upgrade: confuse>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from pandas-profiling) (1.0.0)\n",
            "Requirement already satisfied, skipping upgrade: retrying>=1.3.3 in /usr/local/lib/python3.6/dist-packages (from plotly) (1.3.3)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from plotly) (1.12.0)\n",
            "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.20.0->category_encoders) (0.13.2)\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil>=2.5.0 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.21.1->category_encoders) (2.5.3)\n",
            "Requirement already satisfied, skipping upgrade: pytz>=2011k in /usr/local/lib/python3.6/dist-packages (from pandas>=0.21.1->category_encoders) (2018.9)\n",
            "Requirement already satisfied, skipping upgrade: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2>=2.8->pandas-profiling) (1.1.1)\n",
            "Requirement already satisfied, skipping upgrade: nbconvert>=5.3.1 in /usr/local/lib/python3.6/dist-packages (from phik>=0.9.8->pandas-profiling) (5.5.0)\n",
            "Requirement already satisfied, skipping upgrade: pytest-pylint>=0.13.0 in /usr/local/lib/python3.6/dist-packages (from phik>=0.9.8->pandas-profiling) (0.14.1)\n",
            "Requirement already satisfied, skipping upgrade: pytest>=4.0.2 in /usr/local/lib/python3.6/dist-packages (from phik>=0.9.8->pandas-profiling) (5.0.1)\n",
            "Requirement already satisfied, skipping upgrade: numba>=0.38.1 in /usr/local/lib/python3.6/dist-packages (from phik>=0.9.8->pandas-profiling) (0.40.1)\n",
            "Requirement already satisfied, skipping upgrade: jupyter-client>=5.2.3 in /usr/local/lib/python3.6/dist-packages (from phik>=0.9.8->pandas-profiling) (5.3.1)\n",
            "Requirement already satisfied, skipping upgrade: seaborn in /usr/local/lib/python3.6/dist-packages (from missingno>=0.4.2->pandas-profiling) (0.9.0)\n",
            "Requirement already satisfied, skipping upgrade: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=1.4->pandas-profiling) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=1.4->pandas-profiling) (0.10.0)\n",
            "Requirement already satisfied, skipping upgrade: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=1.4->pandas-profiling) (2.4.2)\n",
            "Requirement already satisfied, skipping upgrade: pyyaml in /usr/local/lib/python3.6/dist-packages (from confuse>=1.0.0->pandas-profiling) (3.13)\n",
            "Requirement already satisfied, skipping upgrade: nbformat>=4.4 in /usr/local/lib/python3.6/dist-packages (from nbconvert>=5.3.1->phik>=0.9.8->pandas-profiling) (4.4.0)\n",
            "Requirement already satisfied, skipping upgrade: jupyter-core in /usr/local/lib/python3.6/dist-packages (from nbconvert>=5.3.1->phik>=0.9.8->pandas-profiling) (4.5.0)\n",
            "Requirement already satisfied, skipping upgrade: testpath in /usr/local/lib/python3.6/dist-packages (from nbconvert>=5.3.1->phik>=0.9.8->pandas-profiling) (0.4.2)\n",
            "Requirement already satisfied, skipping upgrade: mistune>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert>=5.3.1->phik>=0.9.8->pandas-profiling) (0.8.4)\n",
            "Requirement already satisfied, skipping upgrade: entrypoints>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from nbconvert>=5.3.1->phik>=0.9.8->pandas-profiling) (0.3)\n",
            "Requirement already satisfied, skipping upgrade: pandocfilters>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert>=5.3.1->phik>=0.9.8->pandas-profiling) (1.4.2)\n",
            "Requirement already satisfied, skipping upgrade: bleach in /usr/local/lib/python3.6/dist-packages (from nbconvert>=5.3.1->phik>=0.9.8->pandas-profiling) (3.1.0)\n",
            "Requirement already satisfied, skipping upgrade: traitlets>=4.2 in /usr/local/lib/python3.6/dist-packages (from nbconvert>=5.3.1->phik>=0.9.8->pandas-profiling) (4.3.2)\n",
            "Requirement already satisfied, skipping upgrade: defusedxml in /usr/local/lib/python3.6/dist-packages (from nbconvert>=5.3.1->phik>=0.9.8->pandas-profiling) (0.6.0)\n",
            "Requirement already satisfied, skipping upgrade: pygments in /usr/local/lib/python3.6/dist-packages (from nbconvert>=5.3.1->phik>=0.9.8->pandas-profiling) (2.1.3)\n",
            "Requirement already satisfied, skipping upgrade: pylint>=1.4.5 in /usr/local/lib/python3.6/dist-packages (from pytest-pylint>=0.13.0->phik>=0.9.8->pandas-profiling) (2.3.1)\n",
            "Requirement already satisfied, skipping upgrade: packaging in /usr/local/lib/python3.6/dist-packages (from pytest>=4.0.2->phik>=0.9.8->pandas-profiling) (19.1)\n",
            "Requirement already satisfied, skipping upgrade: importlib-metadata>=0.12 in /usr/local/lib/python3.6/dist-packages (from pytest>=4.0.2->phik>=0.9.8->pandas-profiling) (0.19)\n",
            "Requirement already satisfied, skipping upgrade: py>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from pytest>=4.0.2->phik>=0.9.8->pandas-profiling) (1.8.0)\n",
            "Requirement already satisfied, skipping upgrade: atomicwrites>=1.0 in /usr/local/lib/python3.6/dist-packages (from pytest>=4.0.2->phik>=0.9.8->pandas-profiling) (1.3.0)\n",
            "Requirement already satisfied, skipping upgrade: wcwidth in /usr/local/lib/python3.6/dist-packages (from pytest>=4.0.2->phik>=0.9.8->pandas-profiling) (0.1.7)\n",
            "Requirement already satisfied, skipping upgrade: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest>=4.0.2->phik>=0.9.8->pandas-profiling) (19.1.0)\n",
            "Requirement already satisfied, skipping upgrade: more-itertools>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pytest>=4.0.2->phik>=0.9.8->pandas-profiling) (7.2.0)\n",
            "Requirement already satisfied, skipping upgrade: pluggy<1.0,>=0.12 in /usr/local/lib/python3.6/dist-packages (from pytest>=4.0.2->phik>=0.9.8->pandas-profiling) (0.12.0)\n",
            "Requirement already satisfied, skipping upgrade: llvmlite>=0.25.0dev0 in /usr/local/lib/python3.6/dist-packages (from numba>=0.38.1->phik>=0.9.8->pandas-profiling) (0.29.0)\n",
            "Requirement already satisfied, skipping upgrade: tornado>=4.1 in /usr/local/lib/python3.6/dist-packages (from jupyter-client>=5.2.3->phik>=0.9.8->pandas-profiling) (4.5.3)\n",
            "Requirement already satisfied, skipping upgrade: pyzmq>=13 in /usr/local/lib/python3.6/dist-packages (from jupyter-client>=5.2.3->phik>=0.9.8->pandas-profiling) (17.0.0)\n",
            "Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib>=1.4->pandas-profiling) (41.0.1)\n",
            "Requirement already satisfied, skipping upgrade: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from nbformat>=4.4->nbconvert>=5.3.1->phik>=0.9.8->pandas-profiling) (0.2.0)\n",
            "Requirement already satisfied, skipping upgrade: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.6/dist-packages (from nbformat>=4.4->nbconvert>=5.3.1->phik>=0.9.8->pandas-profiling) (2.6.0)\n",
            "Requirement already satisfied, skipping upgrade: webencodings in /usr/local/lib/python3.6/dist-packages (from bleach->nbconvert>=5.3.1->phik>=0.9.8->pandas-profiling) (0.5.1)\n",
            "Requirement already satisfied, skipping upgrade: decorator in /usr/local/lib/python3.6/dist-packages (from traitlets>=4.2->nbconvert>=5.3.1->phik>=0.9.8->pandas-profiling) (4.4.0)\n",
            "Requirement already satisfied, skipping upgrade: mccabe<0.7,>=0.6 in /usr/local/lib/python3.6/dist-packages (from pylint>=1.4.5->pytest-pylint>=0.13.0->phik>=0.9.8->pandas-profiling) (0.6.1)\n",
            "Requirement already satisfied, skipping upgrade: astroid<3,>=2.2.0 in /usr/local/lib/python3.6/dist-packages (from pylint>=1.4.5->pytest-pylint>=0.13.0->phik>=0.9.8->pandas-profiling) (2.2.5)\n",
            "Requirement already satisfied, skipping upgrade: isort<5,>=4.2.5 in /usr/local/lib/python3.6/dist-packages (from pylint>=1.4.5->pytest-pylint>=0.13.0->phik>=0.9.8->pandas-profiling) (4.3.21)\n",
            "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.12->pytest>=4.0.2->phik>=0.9.8->pandas-profiling) (0.5.2)\n",
            "Requirement already satisfied, skipping upgrade: lazy-object-proxy in /usr/local/lib/python3.6/dist-packages (from astroid<3,>=2.2.0->pylint>=1.4.5->pytest-pylint>=0.13.0->phik>=0.9.8->pandas-profiling) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: wrapt in /usr/local/lib/python3.6/dist-packages (from astroid<3,>=2.2.0->pylint>=1.4.5->pytest-pylint>=0.13.0->phik>=0.9.8->pandas-profiling) (1.11.2)\n",
            "Requirement already satisfied, skipping upgrade: typed-ast>=1.3.0; implementation_name == \"cpython\" in /usr/local/lib/python3.6/dist-packages (from astroid<3,>=2.2.0->pylint>=1.4.5->pytest-pylint>=0.13.0->phik>=0.9.8->pandas-profiling) (1.4.0)\n",
            "Reinitialized existing Git repository in /content/.git/\n",
            "fatal: remote origin already exists.\n",
            "From https://github.com/hurshd0/DS-Unit-2-Regression-Classification\n",
            " * branch            master     -> FETCH_HEAD\n",
            "Already up to date.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ipBYS77PUwNR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Ignore this Numpy warning when using Plotly Express:\n",
        "# FutureWarning: Method .ptp is deprecated and will be removed in a \n",
        "# future version. Use numpy.ptp instead.\n",
        "import warnings\n",
        "warnings.filterwarnings(action='ignore', category=FutureWarning, module='numpy')\n",
        "\n",
        "\n",
        "################################ EDA IMPORTS ###################################\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pandas_profiling\n",
        "import matplotlib.pyplot as plt # plotting lib\n",
        "import seaborn as sns # matplotlib wrapper plotting lib\n",
        "import plotly.graph_objs as go # interactive low-level plotting lib https://plot.ly/python/\n",
        "import plotly.express as px #high-level api wrapper for plotly https://plot.ly/python/plotly-express/#visualize-distributions\n",
        "\n",
        "# ---------------- Plot libs settings ------------- #\n",
        "plt.style.use('seaborn-darkgrid')\n",
        "sns.set(context='notebook', style='darkgrid', palette='colorblind')\n",
        "\n",
        "# ---------------- Pandas settings --------------- #\n",
        "# Removes rows and columns truncation of '...'\n",
        "pd.set_option('display.max_rows', 500)\n",
        "pd.set_option('display.max_columns', 500)\n",
        "pd.set_option('display.width', 1000)\n",
        "################################################################################\n",
        "\n",
        "################################ STATS IMPORTS #################################\n",
        "\n",
        "################################################################################\n",
        "\n",
        "\n",
        "#################################### ML IMPORTS ################################\n",
        "import category_encoders as ce\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.feature_selection import f_regression, SelectKBest\n",
        "################################################################################"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QJBD4ruICm1m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#### HELPER FUNCTIONS ######\n",
        "\n",
        "def correlations(data, y, xs):\n",
        "    from scipy import stats\n",
        "    rs = []\n",
        "    rhos = []\n",
        "    for x in xs:\n",
        "        r = stats.pearsonr(data[y], data[x])[0]\n",
        "        rs.append(r)\n",
        "        rho = stats.spearmanr(data[y], data[x])[0]\n",
        "        rhos.append(rho)\n",
        "    return pd.DataFrame({\"feature\": xs, \"r\": rs, \"rho\": rhos})\n",
        "\n",
        "def correlation_heatmap(data=None, vmax=1, annot=True, corr_type='pearson', figsize=(12, 12)):\n",
        "    if data is None:\n",
        "        raise ValueError(\n",
        "            \"The parameter 'data' must be assigned a non-nil reference to a Pandas DataFrame\")\n",
        "    # Taken from the seaborn example at:\n",
        "    # http://seaborn.pydata.org/examples/many_pairwise_correlations.html\n",
        "    # Compute the correlation matrix\n",
        "    corr = data.corr(corr_type)\n",
        "    # Generate a mask for the upper triangle\n",
        "    mask = np.zeros_like(corr, dtype=np.bool)\n",
        "    mask[np.triu_indices_from(mask)] = True\n",
        "    # Set up the matplotlib figure\n",
        "    fig, axes = plt.subplots(figsize=figsize)\n",
        "    # Generate a custom diverging colormap\n",
        "    cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
        "    # Draw the heatmap with the mask and correct aspect ratio\n",
        "    sns.heatmap(corr, mask=mask, cmap=cmap, vmax=vmax, annot=annot, square=True,\n",
        "                linewidths=.5, cbar_kws={\"shrink\": .5}, fmt=',.2f', ax=axes)\n",
        "    plt.show()\n",
        "    plt.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "STg53u1MbvBg",
        "colab_type": "text"
      },
      "source": [
        "# LOAD DATESET"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "riO1yV5PFYLb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "outputId": "22134cc5-326f-48bb-a7d1-38f008c6b170"
      },
      "source": [
        "def load_nyc_rolling_sales_data(filepath):\n",
        "    # Read New York City property sales data\n",
        "    df = pd.read_csv(filepath)\n",
        "\n",
        "    # Change column names: replace spaces with underscores\n",
        "    df.columns = [col.replace(' ', '_') for col in df]\n",
        "\n",
        "    # SALE_PRICE was read as strings.\n",
        "    # Remove symbols, convert to integer\n",
        "    df['SALE_PRICE'] = (\n",
        "        df['SALE_PRICE']\n",
        "        .str.replace('$','')\n",
        "        .str.replace('-','')\n",
        "        .str.replace(',','')\n",
        "        .astype(int)\n",
        "    )\n",
        "\n",
        "    # Keep subset of rows:\n",
        "    # 1. BUILDING_CLASS_CATEGORY == '01 ONE FAMILY DWELLINGS'\n",
        "    # 2. SALE_PRICE > $100K AND SALE_PRICE < $2M\n",
        "\n",
        "    mask = (\n",
        "        (df['BUILDING_CLASS_CATEGORY'] == '01 ONE FAMILY DWELLINGS') &\n",
        "        ((df['SALE_PRICE'] >= 100000) & (df['SALE_PRICE'] <= 2000000))\n",
        "    )\n",
        "    df = df[mask]\n",
        "\n",
        "    # Drop unnecessary columns or columns that have > 90% NaNs\n",
        "    df = df.drop(['ADDRESS', 'BUILDING_CLASS_CATEGORY','EASE-MENT','APARTMENT_NUMBER', 'TAX_CLASS_AT_PRESENT', 'TAX_CLASS_AT_TIME_OF_SALE'], axis=1)\n",
        "\n",
        "    # Convert SALE_DATE, YEAR_BUILT to datetime\n",
        "    df['SALE_DATE'] = pd.to_datetime(df['SALE_DATE'], infer_datetime_format=True)\n",
        "    df['YEAR_BUILT'] = pd.to_datetime(df['YEAR_BUILT'], infer_datetime_format=True).dt.year\n",
        "\n",
        "    # Convert LAND_SQUARE_FEET to float\n",
        "    df['LAND_SQUARE_FEET'] = df['LAND_SQUARE_FEET'].str.replace(',', '').astype(float)\n",
        "\n",
        "    # Convert float to int\n",
        "    float_cols = ['TOTAL_UNITS', 'RESIDENTIAL_UNITS', 'COMMERCIAL_UNITS']\n",
        "    for col in float_cols:\n",
        "        df[col] = df[col].astype(int)\n",
        "\n",
        "    # Convert to categorical\n",
        "    cat_cols = ['BOROUGH', 'TOTAL_UNITS', 'RESIDENTIAL_UNITS', 'COMMERCIAL_UNITS']\n",
        "    for col in cat_cols:\n",
        "        df[col] = df[col].astype(object)\n",
        "    \n",
        "    # Map BOROUGHS to their names\n",
        "    boroughs_name = {1:'MANHATTAN', 2:'BROOKLYN', 3:'QUEENS', 4:'BRONX', 5:'STATEN ISLAND'}\n",
        "    df['BOROUGH'] = df['BOROUGH'].map(boroughs_name)\n",
        "    return df\n",
        "    \n",
        "raw_df = load_nyc_rolling_sales_data('../data/NYC_Citywide_Rolling_Calendar_Sales.csv')\n",
        "raw_df.info()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 3164 entries, 44 to 23035\n",
            "Data columns (total 15 columns):\n",
            "BOROUGH                           3164 non-null object\n",
            "NEIGHBORHOOD                      3164 non-null object\n",
            "BLOCK                             3164 non-null int64\n",
            "LOT                               3164 non-null int64\n",
            "BUILDING_CLASS_AT_PRESENT         3164 non-null object\n",
            "ZIP_CODE                          3164 non-null float64\n",
            "RESIDENTIAL_UNITS                 3164 non-null object\n",
            "COMMERCIAL_UNITS                  3164 non-null object\n",
            "TOTAL_UNITS                       3164 non-null object\n",
            "LAND_SQUARE_FEET                  3164 non-null float64\n",
            "GROSS_SQUARE_FEET                 3164 non-null float64\n",
            "YEAR_BUILT                        3164 non-null int64\n",
            "BUILDING_CLASS_AT_TIME_OF_SALE    3164 non-null object\n",
            "SALE_PRICE                        3164 non-null int64\n",
            "SALE_DATE                         3164 non-null datetime64[ns]\n",
            "dtypes: datetime64[ns](1), float64(3), int64(4), object(7)\n",
            "memory usage: 395.5+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_4lVwBMwmYD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get Pandas Profiling Report\n",
        "# raw_df.profile_report()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RKttPgqDwmU4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 348
        },
        "outputId": "c45e4e06-8d1a-4637-e264-5371293fdea8"
      },
      "source": [
        "raw_df.head()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>BOROUGH</th>\n",
              "      <th>NEIGHBORHOOD</th>\n",
              "      <th>BLOCK</th>\n",
              "      <th>LOT</th>\n",
              "      <th>BUILDING_CLASS_AT_PRESENT</th>\n",
              "      <th>ZIP_CODE</th>\n",
              "      <th>RESIDENTIAL_UNITS</th>\n",
              "      <th>COMMERCIAL_UNITS</th>\n",
              "      <th>TOTAL_UNITS</th>\n",
              "      <th>LAND_SQUARE_FEET</th>\n",
              "      <th>GROSS_SQUARE_FEET</th>\n",
              "      <th>YEAR_BUILT</th>\n",
              "      <th>BUILDING_CLASS_AT_TIME_OF_SALE</th>\n",
              "      <th>SALE_PRICE</th>\n",
              "      <th>SALE_DATE</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>QUEENS</td>\n",
              "      <td>OCEAN PARKWAY-NORTH</td>\n",
              "      <td>5495</td>\n",
              "      <td>801</td>\n",
              "      <td>A9</td>\n",
              "      <td>11230.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>6800.0</td>\n",
              "      <td>1325.0</td>\n",
              "      <td>1970</td>\n",
              "      <td>A9</td>\n",
              "      <td>550000</td>\n",
              "      <td>2019-01-01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>61</th>\n",
              "      <td>BRONX</td>\n",
              "      <td>QUEENS VILLAGE</td>\n",
              "      <td>7918</td>\n",
              "      <td>72</td>\n",
              "      <td>A1</td>\n",
              "      <td>11427.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>4000.0</td>\n",
              "      <td>2001.0</td>\n",
              "      <td>1970</td>\n",
              "      <td>A1</td>\n",
              "      <td>200000</td>\n",
              "      <td>2019-01-01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>78</th>\n",
              "      <td>BROOKLYN</td>\n",
              "      <td>PELHAM PARKWAY SOUTH</td>\n",
              "      <td>4210</td>\n",
              "      <td>19</td>\n",
              "      <td>A1</td>\n",
              "      <td>10461.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>3500.0</td>\n",
              "      <td>2043.0</td>\n",
              "      <td>1970</td>\n",
              "      <td>A1</td>\n",
              "      <td>810000</td>\n",
              "      <td>2019-01-02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>108</th>\n",
              "      <td>QUEENS</td>\n",
              "      <td>FLATBUSH-CENTRAL</td>\n",
              "      <td>5212</td>\n",
              "      <td>69</td>\n",
              "      <td>A1</td>\n",
              "      <td>11226.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>4000.0</td>\n",
              "      <td>2680.0</td>\n",
              "      <td>1970</td>\n",
              "      <td>A1</td>\n",
              "      <td>125000</td>\n",
              "      <td>2019-01-02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>111</th>\n",
              "      <td>QUEENS</td>\n",
              "      <td>FLATBUSH-EAST</td>\n",
              "      <td>7930</td>\n",
              "      <td>121</td>\n",
              "      <td>A5</td>\n",
              "      <td>11203.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1710.0</td>\n",
              "      <td>1872.0</td>\n",
              "      <td>1970</td>\n",
              "      <td>A5</td>\n",
              "      <td>620000</td>\n",
              "      <td>2019-01-02</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      BOROUGH          NEIGHBORHOOD  BLOCK  LOT BUILDING_CLASS_AT_PRESENT  ZIP_CODE RESIDENTIAL_UNITS COMMERCIAL_UNITS TOTAL_UNITS  LAND_SQUARE_FEET  GROSS_SQUARE_FEET  YEAR_BUILT BUILDING_CLASS_AT_TIME_OF_SALE  SALE_PRICE  SALE_DATE\n",
              "44     QUEENS   OCEAN PARKWAY-NORTH   5495  801                        A9   11230.0                 1                0           1            6800.0             1325.0        1970                             A9      550000 2019-01-01\n",
              "61      BRONX        QUEENS VILLAGE   7918   72                        A1   11427.0                 1                0           1            4000.0             2001.0        1970                             A1      200000 2019-01-01\n",
              "78   BROOKLYN  PELHAM PARKWAY SOUTH   4210   19                        A1   10461.0                 1                0           1            3500.0             2043.0        1970                             A1      810000 2019-01-02\n",
              "108    QUEENS      FLATBUSH-CENTRAL   5212   69                        A1   11226.0                 1                0           1            4000.0             2680.0        1970                             A1      125000 2019-01-02\n",
              "111    QUEENS         FLATBUSH-EAST   7930  121                        A5   11203.0                 1                0           1            1710.0             1872.0        1970                             A5      620000 2019-01-02"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZOcpz-2Oxsc",
        "colab_type": "text"
      },
      "source": [
        "# Split Train-Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6HP779tbO7tI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "34815917-0d0b-4c4d-d6da-6fa3571b23c3"
      },
      "source": [
        "train = raw_df[raw_df['SALE_DATE'] < '2019-04-01']\n",
        "test = raw_df[raw_df['SALE_DATE'] >= '2019-04-01']\n",
        "train.shape, test.shape"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((2517, 15), (647, 15))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dwqW1PyLMjlM",
        "colab_type": "text"
      },
      "source": [
        "# EDA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PVwhfWdJNsvf",
        "colab_type": "text"
      },
      "source": [
        "## PAIRWISE ANALYSIS\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iLVY_MWqPSzg",
        "colab_type": "text"
      },
      "source": [
        "### `GROSS_SQUARE_FEET` vs. `SALE_PRICE` AND `LAND_SQUARE_FEET` vs. `SALE_PRICE`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "idYkmkCLCvUb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "px.scatter(train, x='GROSS_SQUARE_FEET', y='SALE_PRICE', trendline='lowess')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8WEHBLnvPNSS",
        "colab_type": "text"
      },
      "source": [
        "Notice how `SALE_PRICE` is greater than $0 for `GROSS_SQUARE_FEET == 0`, so we can drop them as they might represent data entry error or placeholder for NaN values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lw9F8rHrQJRw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "(train['GROSS_SQUARE_FEET'] == 0).value_counts()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ILHQya8NQI69",
        "colab_type": "text"
      },
      "source": [
        "There are about 31 values that have `0` Gross Square Feet. Let's check if their `LAND_SQUARE_FEET` is 0 as well."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ST6mJPlwRmCF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "(train['LAND_SQUARE_FEET'] == 0).value_counts()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-arCulcRfvQ",
        "colab_type": "text"
      },
      "source": [
        "Let's plot them to see it..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QqibcFy3R9k8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "px.scatter(train, x='LAND_SQUARE_FEET', y='SALE_PRICE', trendline='lowess')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WCQWVK2dSK_C",
        "colab_type": "text"
      },
      "source": [
        "Indeed there are 0s for `LAND_SQUARE_FEET` as well, let's see if the rows are the 0 for `GROSS_SQUARE_FEET` as well."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q1PMPyMwSYFB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train.query('GROSS_SQUARE_FEET == 0 & LAND_SQUARE_FEET == 0')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IpYOxJmeSqZQ",
        "colab_type": "text"
      },
      "source": [
        "Some of the commonalities are:\n",
        "\n",
        "- Both have the same neighborhood - `BREEZY POINT` and `THROGS NECK`\n",
        "- Both have the same block - `2, 4`\n",
        "- Both have the same `BUILDING_CLASS_AT_PRESENT` and `BUILDING_CLASS_AT_TIME_OF_SALE` - `A8`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BfHi2d6HPK3W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Let's drop them \n",
        "train = train[train['GROSS_SQUARE_FEET'] != 0]\n",
        "test = test[test['GROSS_SQUARE_FEET'] != 0]\n",
        "train.shape, test.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HWVjrUu9Uj2J",
        "colab_type": "text"
      },
      "source": [
        "### `BOROUGH` vs `PRICE`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nbDX9473Wcg-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train.groupby('BOROUGH')['SALE_PRICE'].describe()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_llfGBzrYYt-",
        "colab_type": "text"
      },
      "source": [
        "We can see the mean and median sale prices are all different for the boroughs, let's plot them to visualize their difference. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ru9bk-fsPK0o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sns.catplot(x='BOROUGH', y='SALE_PRICE', data=train, kind='bar', color='grey', height=6, aspect=1.5);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V3FLOHgkj1lB",
        "colab_type": "text"
      },
      "source": [
        "### `NEIGHBORHOOD` vs. `SALE_PRICE`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KbThxvMAj6eR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train['NEIGHBORHOOD'].value_counts()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QI4TpkctkOYV",
        "colab_type": "text"
      },
      "source": [
        "It looks like their is high cardinality in `NEIGHBORHOOD`, we can reduce cardinality by keeping only top 10, and rest grouped as `OTHER`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ChQBM561j6Ze",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get a list of the top 10 neighborhoods\n",
        "top10 = train['NEIGHBORHOOD'].value_counts()[:10].index\n",
        "\n",
        "# Filter locations based on top10 neighborhoods, and not as OTHERS\n",
        "\n",
        "train.loc[~train['NEIGHBORHOOD'].isin(top10), 'NEIGHBORHOOD'] = 'OTHER'\n",
        "test.loc[~test['NEIGHBORHOOD'].isin(top10), 'NEIGHBORHOOD'] = 'OTHER'\n",
        "\n",
        "sns.catplot(x='NEIGHBORHOOD', y='SALE_PRICE', data=train, color='grey',  kind='bar', height=6, aspect=2);\n",
        "plt.xticks(rotation=45);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7pc5odurj6R4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train['NEIGHBORHOOD'].value_counts()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJ9Ub1SzZpP5",
        "colab_type": "text"
      },
      "source": [
        "### `BUILDING_CLASS` vs. `SALE_PRICE`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7kbuGuhebWLw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sns.catplot(x='BUILDING_CLASS_AT_PRESENT', y='SALE_PRICE', data=train, kind='box', color='grey', height=6, aspect=1.5);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oV93R5PIbWIi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sns.catplot(x='BUILDING_CLASS_AT_TIME_OF_SALE', y='SALE_PRICE', data=train, color='grey', kind='box', height=6, aspect=1.5);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "On5E2f_Uir3D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train.groupby('BUILDING_CLASS_AT_TIME_OF_SALE')['SALE_PRICE'].describe()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZUF01I13iSl9",
        "colab_type": "text"
      },
      "source": [
        "It would be appropriate to choose `BUILDING_CLASS_AT_TIME_OF_SALE` since, it would the most recent `BUILDING_CLASS` related to `SALE_PRICE`.\n",
        "\n",
        "Another thing to note is `SALE_PRICE` of:\n",
        "\n",
        "- `A9`, `A1`, `A5`, `A0`, `A2` are almost similar, mean, and median values compared to `A3`, `A4`, `A6`, `S1` and `S0`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9qK3UUL5cOgg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Let's group A0, A1, A2, A5, A9 in one group and separate A3, A4, A6, S1 and S0\n",
        "# TODO"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1DCpu2NscSfL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train['BUILDING_CLASS_AT_TIME_OF_SALE'].value_counts()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HArsVmgYn8we",
        "colab_type": "text"
      },
      "source": [
        "### `ZIP_CODE` vs. `SALE_PRICE`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DPdbseMMs1V1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sns.lmplot(x='ZIP_CODE', y='SALE_PRICE', data=train, height=6, aspect=1.5, scatter_kws=dict(alpha=0.05));"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QN1kimyqxr5f",
        "colab_type": "text"
      },
      "source": [
        "It looks like we can bin the zipcodes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cud1hP5cs1R8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train['ZIPCODE_CAT'] = pd.cut(train['ZIP_CODE'], [0, 10400, 10480, 11200, 11250, 11400, 11500, 11700], right=True, labels=['ZIP_10400', 'ZIP_10480', 'ZIP_11200', 'ZIP_11250', 'ZIP_11400', 'ZIP_11500', 'ZIP_11700'])\n",
        "test['ZIPCODE_CAT'] = pd.cut(test['ZIP_CODE'], [0, 10400, 10480, 11200, 11250, 11400, 11500, 11700], right=True, labels=['ZIP_10400', 'ZIP_10480', 'ZIP_11200', 'ZIP_11250', 'ZIP_11400', 'ZIP_11500', 'ZIP_11700'])\n",
        "sns.catplot(x='ZIPCODE_CAT', y='SALE_PRICE', data=train, height=6, aspect=1.5, color='grey')\n",
        "plt.xticks(rotation=45);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wNyvJULg9X2P",
        "colab_type": "text"
      },
      "source": [
        "After binning the zipcodes, it looks like the prices are now different with regards to their bins. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JAM2fp8HPvht",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#### DON'T FORGET TO DROP ZIP_CODE ####\n",
        "train['ZIPCODE_CAT'] = train['ZIPCODE_CAT'].astype(object)\n",
        "test['ZIPCODE_CAT'] = test['ZIPCODE_CAT'].astype(object)\n",
        "train = train.drop('ZIP_CODE', axis=1)\n",
        "test = test.drop('ZIP_CODE', axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_3H-whPGZpHi",
        "colab_type": "text"
      },
      "source": [
        "### `YEAR_BUILT` vs. `SALE_PRICE`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x7_T4_Xc_Ktc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train['YEAR_BUILT'].value_counts()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7JJ0zCdH_YXB",
        "colab_type": "text"
      },
      "source": [
        "Since, all the values are the same we can drop the column."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dlaXIIex_Khv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train = train.drop('YEAR_BUILT', axis=1)\n",
        "test = test.drop('YEAR_BUILT', axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5Do_1uVX_stL"
      },
      "source": [
        "### `BLOCK` vs. `SALE_PRICE`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZ-CdoTkBV2f",
        "colab_type": "text"
      },
      "source": [
        "`BLOCK:` \n",
        "\n",
        "> A Tax Block is a sub-division of the borough on which real properties are located. The  Department  of  Finance  uses  a  Borough-Block-Lot  classification  to  label  all  real  property  in  the  City."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "68Or3vqkAd-f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train['BLOCK'].describe()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "iSE0Z78u_ss4",
        "colab": {}
      },
      "source": [
        "sns.lmplot(x='BLOCK', y='SALE_PRICE', data=train, height=6, aspect=1.5, scatter_kws=dict(alpha=0.05));"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NxtQbmrfBM3H",
        "colab_type": "text"
      },
      "source": [
        "It doesn't look like `BLOCK` is a good predictor of `SALE_PRICE`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F0RhO8FEBqzW",
        "colab_type": "text"
      },
      "source": [
        "### `LOT` vs. `SALE_PRICE`\n",
        "\n",
        "`LOT:`\n",
        "\n",
        "> A Tax Lot is a subdivision of a Tax Block and represents the property unique location."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a2UTSRRN_v6a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sns.lmplot(x='LOT', y='SALE_PRICE', data=train, height=6, aspect=1.5, scatter_kws=dict(alpha=0.05));"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0GH1Gk2nB5j6",
        "colab_type": "text"
      },
      "source": [
        "Again, both `BLOCK` and `LOT` are not good predictors of `SALE_PRICE`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "beU9HROhCBN-",
        "colab_type": "text"
      },
      "source": [
        "### `TOTAL_UNITS, RESIDENTIAL_UNITS, COMMERCIAL_UNITS vs. SALE_PRICE`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZC6V2g5vGg1j",
        "colab_type": "text"
      },
      "source": [
        "First, let's verify if the `RESIDENTIAL_UNITS` + `COMMERICAL_UNITS` = `TOTAL_UNITS`\n",
        "\n",
        "if it is, we can just discard other, and keep `TOTAL_UNITS`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DC-9lf5PEW5u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "(train['RESIDENTIAL_UNITS'] + train['COMMERCIAL_UNITS']).value_counts()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nM4wE52CGa5l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train['TOTAL_UNITS'].value_counts()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zeQPxtMEETXr",
        "colab_type": "text"
      },
      "source": [
        "Indeed, it is the same, let's keep `TOTAL_UNITS`, and drop the others out."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ydIINBVJCxxF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train = train.drop(['RESIDENTIAL_UNITS', 'COMMERCIAL_UNITS'], axis=1)\n",
        "test = test.drop(['RESIDENTIAL_UNITS', 'COMMERCIAL_UNITS'], axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3TPu-eyWCxu_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sns.catplot(x='TOTAL_UNITS', y='SALE_PRICE', data=train, kind='bar', color='grey', height=6, aspect=1.5);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lwoOOgqDHSP3",
        "colab_type": "text"
      },
      "source": [
        "We can see `TOTAL_UNITS` is good indicator of `SALE_PRICE`. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iXDX6MCRWJVO",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pwo8HZ5YOh2v",
        "colab_type": "text"
      },
      "source": [
        "# ONE-HOT ENCODINGS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sRiUuIBROpwR",
        "colab_type": "text"
      },
      "source": [
        "Let's try out one-hot encodings of chosen cateogrical features:\n",
        "\n",
        "- `TOTAL_UNITS` \n",
        "- `ZIPCODE_CAT`\n",
        "- `BOROUGH`\n",
        "- `NEIGHBORHOOD`\n",
        "- `BUILDING_CLASS_AT_TIME_OF_SALE`\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SnlpLc9TO119",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train.describe(exclude='number').T.sort_values(by='unique')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VIjKY-PfQK41",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoder = ce.OneHotEncoder(use_cat_names=True)\n",
        "cat_features = ['TOTAL_UNITS', 'BOROUGH', 'ZIPCODE_CAT', 'NEIGHBORHOOD', 'BUILDING_CLASS_AT_TIME_OF_SALE']\n",
        "numeric_features = ['GROSS_SQUARE_FEET', 'LAND_SQUARE_FEET', 'SALE_PRICE']\n",
        "combined_features = cat_features + numeric_features\n",
        "train_encoded = encoder.fit_transform(train[combined_features])\n",
        "test_encoded = encoder.transform(test[combined_features])\n",
        "train_encoded.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fdB8i_0fR7rS",
        "colab_type": "text"
      },
      "source": [
        "So, encodings work great, there was no issue with it, let's check correlations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vIbjd3bmL-AA",
        "colab_type": "text"
      },
      "source": [
        "# CORRELATIONS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oq2MKs1PL1XY",
        "colab_type": "text"
      },
      "source": [
        "Let's summarize what we know so far:\n",
        "\n",
        "- Good Predictors:\n",
        "  - `TOTAL_UNITS`\n",
        "  - `GROSS_SQUARE_FEET` and `LAND_SQUARE_FEET`\n",
        "  - `BOROUGH`\n",
        "  - `NEIGHBORHOOD`\n",
        "  - `ZIPCODE_CAT`\n",
        "  - May be `BUILDING_CLASS_AT_TIME_OF_SALE` if we bin it.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ciWtxDMha76i",
        "colab_type": "text"
      },
      "source": [
        "## Correlation Heatmap"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Bd4hbFXa94n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "correlation_heatmap(train_encoded, figsize=(25, 25))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OgllB1H8VDaY",
        "colab_type": "text"
      },
      "source": [
        "### Important Correlations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j-gjdY4YMFCv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "correlations(train, 'SALE_PRICE', ['GROSS_SQUARE_FEET', 'LAND_SQUARE_FEET', 'TOTAL_UNITS'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Om1Ye1eWDak9",
        "colab_type": "text"
      },
      "source": [
        "# Linear Regression Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wnolVMtPWZAc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def linear_model_results(data=None, features=None, target=None, one_hot=False, card_n=12, scaling=False, kbest=False, k_val=10, by_date=False, date_col=None, cutoff=None, seed=12345):\n",
        "    if data is None:\n",
        "        raise ValueError( \"The parameter 'data' must be assigned a non-nil reference to a Pandas DataFrame\")\n",
        "    if (features is None) or (not isinstance(features, list)):\n",
        "        raise ValueError( \"The parameter 'features' must be a non-nil reference and list data type\")\n",
        "    if (target is None) or (not isinstance(target, str)):\n",
        "        raise ValueError( \"The parameter 'target' must be a non-nil reference and string object\")\n",
        "    \n",
        "    # Import to split the data into training/testing sets\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    # Import Linear Model\n",
        "    from sklearn.linear_model import LinearRegression\n",
        "    # Import metrics\n",
        "    from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "    # Extract features \n",
        "    if by_date and cutoff and date_col:\n",
        "        cutoff = pd.to_datetime(cutoff)\n",
        "        train = data[data[date_col] < cutoff]\n",
        "        test = data[data[date_col] >= cutoff]\n",
        "        X_train = train[features]\n",
        "        y_train = train[target]\n",
        "        X_test = test[features]\n",
        "        y_test = test[target]\n",
        "    else:\n",
        "        # Train-Test Split\n",
        "        X = df[features]\n",
        "        y = df[target]\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=seed)\n",
        "    \n",
        "    # One-hot encoding only low card. features\n",
        "    if one_hot:\n",
        "        # Check if features have low cardinality otherwise raise error\n",
        "        if any(X_train.describe(exclude='number').T['unique'] > 12):\n",
        "            raise ValueError(f'Cannot One-Hot encode, cardinality of one of the feature is greater than set cutoff {card_n}')\n",
        "        # Import categorical encoder\n",
        "        import category_encoders as ce\n",
        "        # Do one-hot encoding\n",
        "        encoder = ce.OneHotEncoder(use_cat_names=True)\n",
        "        X_train = encoder.fit_transform(X_train)\n",
        "        X_test = encoder.transform(X_test)\n",
        "        # Assert columns are same\n",
        "        assert X_train.shape[1] == X_test.shape[1]\n",
        "\n",
        "    # Standard Scaling features\n",
        "    if scaling:\n",
        "        # Import standard scaler\n",
        "        from sklearn.preprocessing import StandardScaler\n",
        "        scaler = StandardScaler()\n",
        "        X_train = scaler.fit_transform(X_train)\n",
        "        X_test = scaler.transform(X_test)\n",
        "        # Assert columns are same\n",
        "        assert X_train.shape[1] == X_test.shape[1]\n",
        "\n",
        "    # Select K Best features\n",
        "    if kbest and k:\n",
        "        # Select the 10 features that best correlate with the target\n",
        "        from sklearn.feature_selection import f_regression, SelectKBest\n",
        "        selector = SelectKBest(score_func=f_regression, k=10)\n",
        "        X_train = selector.fit_transform(X_train, y_train)\n",
        "        X_test = selector.transform(X_test)\n",
        "        assert X_train.shape[1] == X_test.shape[1]\n",
        "    \n",
        "    # Assign empty dictionary to hold results\n",
        "    results = {}\n",
        "    results['n'] = data.shape[0]\n",
        "\n",
        "    # Instantiate Linear Regression Mode\n",
        "    model = LinearRegression().fit(X_train, y_train)\n",
        "    # Add model to results\n",
        "    results['model'] = model\n",
        "    # Add coefficients to results\n",
        "    results['coefficients'] = model.coef_[0]\n",
        "\n",
        "    # Calculate metrics on training data\n",
        "    # Add R^2 \n",
        "    results['train_r_squared'] = model.score(X_train, y_train)\n",
        "    # Predict\n",
        "    y_train_hat = model.predict(X_train)\n",
        "    # MSE\n",
        "    results['train_MSE'] = mean_squared_error(y_train, y_train_hat)\n",
        "    # RMSE\n",
        "    results['train_RMSE'] = np.sqrt(results['train_MSE'])\n",
        "    # MAE\n",
        "    results['train_MAE'] = mean_absolute_error(y_train, y_train_hat)\n",
        "\n",
        "    # Calculate metrics on the test data\n",
        "    # Add R^2 \n",
        "    results['test_r_squared'] = model.score(X_test, y_test)\n",
        "    # Predict\n",
        "    y_test_hat = model.predict(X_test)\n",
        "    # MSE\n",
        "    results['test_MSE'] = mean_squared_error(y_test, y_test_hat)\n",
        "    # RMSE\n",
        "    results['test_RMSE'] = np.sqrt(results['test_MSE'])\n",
        "    # MAE\n",
        "    results['test_MAE'] = mean_absolute_error(y_test, y_test_hat)\n",
        "    return results\n",
        "\n",
        "\n",
        "def lmodel(X_train, X_test, y_train, y_test):\n",
        "    # Instantiate Linear Regression Mode\n",
        "    model = LinearRegression().fit(X_train, y_train)\n",
        "\n",
        "    # Store metrics   \n",
        "    results = {}\n",
        "    # Add model to results\n",
        "    results['model'] = model\n",
        "    # Add coefficients to results\n",
        "    results['coefficients'] = model.coef_[0]\n",
        "    # Calculate metrics on training data\n",
        "    # Add R^2 \n",
        "    results['train_r_squared'] = model.score(X_train, y_train)\n",
        "    # Predict\n",
        "    y_train_hat = model.predict(X_train)\n",
        "    # MSE\n",
        "    results['train_MSE'] = mean_squared_error(y_train, y_train_hat)\n",
        "    # RMSE\n",
        "    results['train_RMSE'] = np.sqrt(results['train_MSE'])\n",
        "    # MAE\n",
        "    results['train_MAE'] = mean_absolute_error(y_train, y_train_hat)\n",
        "\n",
        "    # Calculate metrics on the test data\n",
        "    # Add R^2 \n",
        "    results['test_r_squared'] = model.score(X_test, y_test)\n",
        "    # Predict\n",
        "    y_test_hat = model.predict(X_test)\n",
        "    # MSE\n",
        "    results['test_MSE'] = mean_squared_error(y_test, y_test_hat)\n",
        "    # RMSE\n",
        "    results['test_RMSE'] = np.sqrt(results['test_MSE'])\n",
        "    # MAE\n",
        "    results['test_MAE'] = mean_absolute_error(y_test, y_test_hat)\n",
        "    return results\n",
        "\n",
        "\n",
        "def print_lm(results):\n",
        "    print(\"\"\"\n",
        "    ----------- Linear Regression Model Results -------------\n",
        "    Training Set\n",
        "    R^2: {:.2f} (Explained variance score: 1 is perfect prediction)\n",
        "    MSE: {:.2f}\n",
        "    RMSE: {:.2f}\n",
        "    MAE: ${:.2f}\n",
        "\n",
        "    Test Set\n",
        "    R^2: {:.2f} (Explained variance score: 1 is perfect prediction)\n",
        "    MSE: {:.2f}\n",
        "    RMSE: {:.2f}\n",
        "    MAE: ${:.2f}\n",
        "    \"\"\".format(results['train_r_squared'], results['train_MSE'], results['train_RMSE'], results['train_MAE'], results['test_r_squared'], results['test_MSE'], results['test_RMSE'], results['test_MAE']))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kU8OGPuFDjJt",
        "colab_type": "text"
      },
      "source": [
        "### Simple Baseline Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qqkUYnktAajl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 1. Extract features\n",
        "target = 'SALE_PRICE'\n",
        "features = ['RESIDENTIAL_UNITS', 'COMMERCIAL_UNITS', 'TOTAL_UNITS', 'LAND_SQUARE_FEET', 'GROSS_SQUARE_FEET']\n",
        "\n",
        "# 2. Instantiate Linear Regression Model\n",
        "results = linear_model_results(raw_df, features, target, by_date=True, date_col='SALE_DATE', cutoff='2019-04-01')\n",
        "\n",
        "print_lm(results)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eFXtfJXpjawH",
        "colab_type": "text"
      },
      "source": [
        "### Complex Linear Regression Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H_Dqr9gxnGWL",
        "colab_type": "text"
      },
      "source": [
        "#### 1. Model with One-Hot Encoding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1lvr00Brl-xD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Subset features, and select target\n",
        "cat_features = ['TOTAL_UNITS', 'BOROUGH', 'ZIPCODE_CAT', 'NEIGHBORHOOD', 'BUILDING_CLASS_AT_TIME_OF_SALE']\n",
        "numeric_features = ['GROSS_SQUARE_FEET', 'LAND_SQUARE_FEET', 'SALE_PRICE']\n",
        "features = cat_features + numeric_features\n",
        "target = 'SALE_PRICE'\n",
        "\n",
        "## Split features to X, y - train, test split\n",
        "X_train = train[features]\n",
        "y_train = train[target]\n",
        "\n",
        "X_test = test[features]\n",
        "y_test = test[target]\n",
        "\n",
        "## 1. Do one-hot encoding\n",
        "encoder = ce.OneHotEncoder(use_cat_names=True)\n",
        "X_train_encoded = encoder.fit_transform(X_train)\n",
        "X_test_encoded = encoder.transform(X_test)\n",
        "\n",
        "# Assert columns are same\n",
        "assert X_train_encoded.shape[1] == X_test_encoded.shape[1]\n",
        "\n",
        "results = lmodel(X_train_encoded, X_test_encoded, y_train, y_test)\n",
        "print_lm(results)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v0FlpdpznOaV",
        "colab_type": "text"
      },
      "source": [
        "#### 2. Model with Standard Scaling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k7AFD_-JNtIg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## 2. Standard Scaling\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train_encoded)\n",
        "X_test_scaled = scaler.transform(X_test_encoded)\n",
        "\n",
        "# Assert columns are same\n",
        "assert X_train_scaled.shape[1] == X_test_scaled.shape[1]\n",
        "\n",
        "results = lmodel(X_train_scaled, X_test_scaled, y_train, y_test)\n",
        "print_lm(results)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UqzbXVNWnWrU",
        "colab_type": "text"
      },
      "source": [
        "### 3. Model with Select K Best features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OhyoU2EJna7-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## 3. Select K Best features\n",
        "# Select the 10 features that best correlate with the target\n",
        "selector = SelectKBest(score_func=f_regression, k=10)\n",
        "X_train_selected = selector.fit_transform(X_train_scaled, y_train)\n",
        "X_test_selected = selector.transform(X_test_scaled)\n",
        "assert X_train_selected.shape[1] == X_test_selected.shape[1]\n",
        "\n",
        "# Which features were selected?\n",
        "all_names = X_train_selected.columns\n",
        "selected_mask = selector.get_support()\n",
        "selected_names = all_names[selected_mask]\n",
        "\n",
        "print('Features selected:')\n",
        "for name in selected_names:\n",
        "    print(name)\n",
        "\n",
        "results = lmodel(X_train_selected, X_test_selected, y_train, y_test)\n",
        "print_lm(results)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}